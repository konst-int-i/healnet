{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T14:51:40.230723Z",
     "start_time": "2023-10-27T14:51:39.678102Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>.container { width:100% !important; }</style>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "if \"x_perceiver\" not in os.listdir():\n",
    "    os.chdir(\"/home/kh701/pycharm/healnet/\")\n",
    "import torch\n",
    "from torch import nn\n",
    "import multiprocessing\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "import einops\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from healnet.models.explainer import Explainer\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "\n",
    "from healnet.utils import Config, flatten_config\n",
    "from healnet.etl import TCGADataset\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "    \n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T14:51:48.182311Z",
     "start_time": "2023-10-27T14:51:39.778657Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filled 0 missing values with mean\n",
      "Missing values per feature: \n",
      " Series([], dtype: int64)\n",
      "Slides available: 436\n",
      "Omic available: 437\n",
      "Overlap: 436\n",
      "Filtering out 1 samples for which there are no omic data available\n",
      "Dataloader initialised for blca dataset\n",
      "Dataset: BLCA\n",
      "Molecular data shape: (436, 2191)\n",
      "Molecular/Slide match: 436/436\n",
      "Slide level count: 4\n",
      "Slide level dimensions: ((79968, 79653), (19992, 19913), (4998, 4978), (2499, 2489))\n",
      "Slide resize dimensions: w: 1024, h: 1024\n",
      "Sources selected: ['omic']\n",
      "Censored share: 0.539\n",
      "Survival_bin_sizes: {0: 72, 1: 83, 2: 109, 3: 172}\n",
      "Filled 0 missing values with mean\n",
      "Missing values per feature: \n",
      " Series([], dtype: int64)\n",
      "Slides available: 1019\n",
      "Omic available: 1022\n",
      "Overlap: 1019\n",
      "Filtering out 3 samples for which there are no omic data available\n",
      "Dataloader initialised for brca dataset\n",
      "Dataset: BRCA\n",
      "Molecular data shape: (1019, 2922)\n",
      "Molecular/Slide match: 1019/1019\n",
      "Slide level count: 3\n",
      "Slide level dimensions: ((35855, 34985), (8963, 8746), (2240, 2186))\n",
      "Slide resize dimensions: w: 1024, h: 1024\n",
      "Sources selected: ['omic']\n",
      "Censored share: 0.868\n",
      "Survival_bin_sizes: {3: 155, 2: 172, 1: 289, 0: 403}\n"
     ]
    }
   ],
   "source": [
    "# get dataloaders\n",
    "config = Config(\"config/main_gpu.yml\").read()\n",
    "config = flatten_config(config) # TODO - refactor to other \n",
    "\n",
    "blca = TCGADataset(\n",
    "    dataset=\"blca\", \n",
    "    config=config, \n",
    "    level=2, \n",
    "    sources=[\"omic\"]\n",
    ")\n",
    "\n",
    "brca = TCGADataset(\n",
    "    dataset=\"brca\", \n",
    "    config=config, \n",
    "    level=2, \n",
    "    sources=[\"omic\"]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# get tabular data\n",
    "blca_loader = DataLoader(\n",
    "    blca, \n",
    "    batch_size=1, \n",
    "    shuffle=True, \n",
    "    num_workers=multiprocessing.cpu_count()-1\n",
    ")\n",
    "[sample], censorship, event_time, y_disc = next(iter(blca_loader))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-27T14:51:48.625194Z",
     "start_time": "2023-10-27T14:51:48.158921Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 1, 2183])"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-27T14:51:48.696213Z",
     "start_time": "2023-10-27T14:51:48.628610Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tabular self-supervised pre-training\n",
    "\n",
    "To start with, we want to build and encoder-decoder model which trains a cross-attention unit as the encoder, which can later on be deployed in the iterative model. We then want to benchmark the performance with pan-cancer pre-training vs. without pre-training. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from healnet.models.healnet import Attention, PreNorm\n",
    "\n",
    "class AttentionEncoder(nn.Module): \n",
    "    \"\"\"\n",
    "    Simple encoder that uses fourier encoding, pre-norm and cross-attention to encode the input features into a latent array \n",
    "    of size (num_latents x latent_dim). Takes in both the input tensors as well as a randomly initialised latent \n",
    "    array as the input. \n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 input_channels: int,\n",
    "                 latent: torch.Tensor, \n",
    "                 input_axis: int = 1, \n",
    "                 attn_dropout: float = 0.1,\n",
    "                 num_heads: int = 4, \n",
    "                 num_freq_bands: int=8, \n",
    "                 ):    \n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_channels = input_channels\n",
    "        self.input_axis = input_axis\n",
    "        self.attn_dropout = attn_dropout\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        \n",
    "        # fourier_channels = (input_axis * ((num_freq_bands * 2) + 1))\n",
    "        # input_dim = fourier_channels + input_channels\n",
    "        input_dim = input_channels\n",
    "                \n",
    "        latent_dim = latent.shape[-1] # required for PreNorm layer\n",
    "        # simple single attention unit\n",
    "        enc = PreNorm(latent_dim, Attention(latent_dim, input_dim, heads=num_heads, dim_head=num_heads, dropout=attn_dropout), context_dim=input_dim)\n",
    "        \n",
    "        self.layers = nn.ModuleList([enc])\n",
    "        \n",
    "    def forward(self, latent: torch.Tensor, context: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Note: context is the data, x is the latent\n",
    "        Args:\n",
    "            latent: \n",
    "            context: \n",
    "\n",
    "        Returns:\n",
    "\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            latent = layer(x=latent, context=context)\n",
    "        return latent\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-27T14:51:48.793935Z",
     "start_time": "2023-10-27T14:51:48.687876Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The decoder often needs to be different depending on the modality, so let's implement modality-specific decoders while trying to have a relatively general-purpose encoder that we can plug into the pipeline.\n",
    "\n",
    "Note that we may change this later down the line. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T15:37:52.843886Z",
     "start_time": "2023-10-27T15:37:52.776254Z"
    }
   },
   "outputs": [],
   "source": [
    "class TabularDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder suited for tabular data. We use the following: \n",
    "    - Skip connections: faster and more stable training\n",
    "    - Batch normalisation: stabilises the activations and speeds up training\n",
    "    - Activation: Output layer to map back to output dimensions, corresponding to the original data dims\n",
    "    Tries to reconstruct the original input given the latent\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_dim: int, num_latents: int, output_dim: int, method: str = \"dense\"):\n",
    "        super(TabularDecoder, self).__init__()\n",
    "        assert method in [\"dense\", \"conv\"], \"Decoder type not recognised\"\n",
    "        layers = []\n",
    "        \n",
    "        if method == \"dense\": \n",
    "            \n",
    "            # flatten latent array (batch, num_latents, latent_dim) -> (batch, num_latents * latent_dim)\n",
    "            layers.extend([nn.Flatten()]) \n",
    "            out_dims = [256, 512, 1024] # may refactor as hyperparameter later\n",
    "            \n",
    "            in_dim = latent_dim * num_latents\n",
    "            for idx, out_dim in enumerate(out_dims):\n",
    "                \n",
    "                layers.extend([\n",
    "                    nn.Linear(in_features=in_dim, out_features=out_dim), \n",
    "                    nn.LeakyReLU(), \n",
    "                    # nn.BatchNorm1d(hidden_dim, track_running_stats=True), \n",
    "                    nn.Dropout(0.5)\n",
    "                ])\n",
    "                \n",
    "                in_dim = out_dim # update for next layer\n",
    "            \n",
    "            # final layer to reconstruct output\n",
    "            layers.append(nn.Linear(in_dim, output_dim))\n",
    "        \n",
    "        self.decode = nn.Sequential(*layers)\n",
    "        print(self.decode)\n",
    "        \n",
    "    def forward(self, latent: torch.Tensor):\n",
    "        return self.decode(latent)\n",
    "    \n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, putting it all together in the encoder-decoder model\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "from typing import *\n",
    "\n",
    "class TabPretrainer(nn.Module): \n",
    "    \"\"\"\n",
    "    Encoder-decoder model for pre-training tabular data.\n",
    "    # TODO - refactor abstract base class for initialisations \n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 sample: torch.Tensor,\n",
    "                 # input_channels: int,\n",
    "                 latent_shape: List[int],\n",
    "                 input_axis: int = 1,\n",
    "                 attn_dropout: float = 0.1,\n",
    "                 num_heads: int = 4,\n",
    "                 num_freq_bands: int=8,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.input_channels = sample.shape[-1]\n",
    "        self.input_axis = input_axis\n",
    "        self.num_latents, self.latent_dim = latent_shape\n",
    "        self.attn_dropout = attn_dropout\n",
    "        self.num_heads = num_heads\n",
    "        self.num_freq_bands = num_freq_bands\n",
    "        \n",
    "        \n",
    "        # randomly initialise latent\n",
    "        self.latent = nn.Parameter(torch.randn(self.num_latents, self.latent_dim))\n",
    "        \n",
    "        # encoder\n",
    "        self.encoder = AttentionEncoder(\n",
    "            input_channels=self.input_channels, \n",
    "            latent=self.latent, \n",
    "            input_axis=self.input_axis, \n",
    "            attn_dropout=attn_dropout, \n",
    "            num_heads=num_heads, \n",
    "            num_freq_bands=num_freq_bands\n",
    "        )\n",
    "        \n",
    "        # decoder\n",
    "        self.decoder = TabularDecoder(\n",
    "            latent_dim=self.latent_dim, \n",
    "            num_latents=self.num_latents,\n",
    "            output_dim=self.input_channels, \n",
    "            method=\"dense\"\n",
    "        )\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # get batch dim\n",
    "        b = x.shape[0]\n",
    "        \n",
    "        # expand latent to batch size\n",
    "        if len(self.latent.shape) == 2:\n",
    "            self.latent = nn.Parameter(einops.repeat(self.latent, \"n d -> b n d\", b=b))\n",
    "        \n",
    "        # encode\n",
    "        self.latent.data = self.encoder(latent=self.latent, context=x).data + self.latent.data\n",
    "        # print(self.latent.shape)\n",
    "        # decode, reconstructed x\n",
    "        rec_x = self.decoder(self.latent)\n",
    "        return rec_x\n",
    "    \n",
    "    def get_latent(self):\n",
    "        return self.latent"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-27T15:38:29.915893Z",
     "start_time": "2023-10-27T15:38:29.838401Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we need to think about tabular loss functions. Here, we can explore both reconstruction losses and contrastive losses. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "class TabularLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Reconstruction loss functions for tabular data. We use two types which are commonly used with continuous data: \n",
    "    - Mean squared error\n",
    "    - Constrastive loss, measured as cosine distance between the original and reconstructed data\n",
    "    We seek to minimise both objectives.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 method: str = \"mse\",\n",
    "                 reduction: str = \"mean\",\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        assert method in [\"mse\", \"contrastive\"], \"Loss type not recognised\"\n",
    "        self.loss_type = method\n",
    "        self.reduction = reduction\n",
    "        \n",
    "        if method == \"mse\":\n",
    "            self.loss = nn.MSELoss(reduction=reduction)\n",
    "        elif method == \"contrastive\":\n",
    "            self.loss = nn.CosineEmbeddingLoss(reduction=reduction)\n",
    "            \n",
    "    def __call__(self, **kwargs):\n",
    "        return self.loss(**kwargs)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-27T15:04:24.864678Z",
     "start_time": "2023-10-27T15:04:24.802259Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, we write a pre-training loop that we can use for pre-training across cancer sites. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2191 2922 1758\n"
     ]
    }
   ],
   "source": [
    "# get overlap between omic columns\n",
    "col1 = blca.omic_df.columns\n",
    "col2 = brca.omic_df.columns\n",
    "print(len(col1), len(col2), len(set(col1).intersection(col2)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-27T14:51:48.995634Z",
     "start_time": "2023-10-27T14:51:48.933595Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "     age  is_female  AAK1_rnaseq  AATK_rnaseq  ABCB1_rnaseq  ABCG2_rnaseq  \\\n0     63          0      -0.6734      -0.4660        0.8401       -0.2222   \n1     66          0       2.4277      -0.3853        0.1104       -0.2183   \n2     66          0       2.4277      -0.3853        0.1104       -0.2183   \n3     69          0       1.1340      -0.4110        0.1572        0.0752   \n4     59          1      -0.5311       0.1418       -0.0998       -0.2493   \n..   ...        ...          ...          ...           ...           ...   \n432   71          0       2.8284       0.9219       -0.4711       -0.1561   \n433   61          1       0.9422       0.2662       -0.5276       -0.3078   \n434   60          1      -0.3000      -0.5301       -0.5559        0.4720   \n435   62          1       3.2208      -0.2592       -0.8130       -0.1423   \n436   65          0      -0.3658      -0.2651        1.5479       -0.2854   \n\n     ABI1_rnaseq  ABL1_rnaseq  ABL2_rnaseq  ACE_rnaseq  ACKR1_rnaseq  \\\n0         2.2318      -0.8171       0.8051     -0.1250       -0.2976   \n1        -0.0952      -0.6255       0.0970     -0.4911       -0.1779   \n2        -0.0952      -0.6255       0.0970     -0.4911       -0.1779   \n3         0.0566      -1.3448      -0.3876      1.0335       -0.3683   \n4        -0.6956      -0.3696      -0.1672     -0.7257       -0.3450   \n..           ...          ...          ...         ...           ...   \n432      -0.7569      -0.0186       2.2843      1.9383       -0.3507   \n433      -0.5164      -0.3653       1.6644      0.4936       -0.3722   \n434       0.0597      -0.1817       3.6724      0.1842       -0.3892   \n435      -1.2421      -1.4423      -0.6631     -0.9650       -0.3868   \n436      -1.0743       3.2052       0.2885     -0.3852        2.6972   \n\n     ACKR3_rnaseq  ACSL3_rnaseq  ACSL6_rnaseq  ACVR1B_rnaseq  ACVR1C_rnaseq  \\\n0          1.2538       -0.3237       -0.1429         0.5258        -0.0748   \n1         -0.4134       -0.1501       -0.1576        -0.3597         0.4555   \n2         -0.4134       -0.1501       -0.1576        -0.3597         0.4555   \n3         -0.3736       -0.3294       -0.1807         0.8215        -0.7729   \n4         -0.1465        0.2727        0.3077         1.3352         2.3315   \n..            ...           ...           ...            ...            ...   \n432       -0.4448       -0.4321       -0.1794        -1.0555        -0.6594   \n433       -0.5500       -0.3539       -0.1324        -1.1019        -0.7735   \n434       -0.6284        0.2315       -0.1468        -1.1972        -0.5964   \n435        1.9972        0.0008       -0.1583        -1.4766        -0.7790   \n436        0.7650       -0.7979       -0.1453        -0.9347        -0.5309   \n\n     ACVR1_rnaseq  ACVR2A_rnaseq  ACVR2B_rnaseq  ACVRL1_rnaseq  ADAM10_rnaseq  \\\n0         -0.2048        -0.3004         0.2998        -0.6414         1.2149   \n1         -1.0758         0.3252         1.7109        -0.5763         2.5860   \n2         -1.0758         0.3252         1.7109        -0.5763         2.5860   \n3         -0.7901        -0.9142        -0.2716        -0.2526         1.2477   \n4          0.2386         1.6382        -0.2124        -0.7441         0.9661   \n..            ...            ...            ...            ...            ...   \n432        1.4145        -1.0607        -0.2349         0.9236        -0.8023   \n433        0.0458        -1.1459        -0.6342         0.4675        -1.0416   \n434       -0.2927        -0.9101        -0.6431        -0.3869        -0.5002   \n435       -1.2992        -1.0639         0.5145        -0.8358        -0.1595   \n436        0.6657        -0.2331        -0.7919         0.7327        -0.7508   \n\n     ADAM17_rnaseq  ADCK1_rnaseq  ADCK2_rnaseq  ADCK5_rnaseq  ...  UTRN_mut  \\\n0           1.1643        0.3720       -0.2883       -0.1974  ...         1   \n1           1.5608       -0.6966        0.1801       -0.3164  ...         0   \n2           1.5608       -0.6966        0.1801       -0.3164  ...         0   \n3           0.8202       -0.1294        0.7846       -0.2564  ...         0   \n4           0.6493       -1.2289       -0.0261       -0.1046  ...         0   \n..             ...           ...           ...           ...  ...       ...   \n432         1.6165       -1.0423       -1.2719       -0.7105  ...         0   \n433        -0.9597       -1.3496        0.4849        1.6007  ...         0   \n434        -0.0913       -0.6892       -0.6854        1.1884  ...         0   \n435        -0.0378       -0.3054        0.7819        1.9629  ...         0   \n436        -0.2766       -0.1653       -0.3842       -0.4158  ...         0   \n\n     VCAN_mut  VPS13B_mut  VPS13C_mut  VPS13D_mut  WDFY3_mut  WNK1_mut  \\\n0           0           0           0           0          0         0   \n1           0           0           0           0          0         0   \n2           0           0           0           0          0         0   \n3           0           0           0           0          0         0   \n4           0           0           0           0          0         0   \n..        ...         ...         ...         ...        ...       ...   \n432         0           0           1           0          0         0   \n433         0           0           0           0          0         0   \n434         0           0           0           0          0         0   \n435         0           0           0           0          0         0   \n436         0           0           0           0          0         0   \n\n     XIRP2_mut  XIST_mut  ZDBF2_mut  ZFHX3_mut  ZFHX4_mut  ZFP36L1_mut  \\\n0            0         0          0          0          1            0   \n1            0         0          0          0          1            0   \n2            0         0          0          0          1            0   \n3            0         0          0          0          0            0   \n4            0         1          0          1          1            0   \n..         ...       ...        ...        ...        ...          ...   \n432          0         0          0          0          0            0   \n433          1         0          0          0          1            0   \n434          0         0          0          0          0            0   \n435          0         0          0          0          0            1   \n436          0         0          0          0          0            0   \n\n     ZFYVE26_mut  ZFYVE9_mut  ZNF236_mut  ZNF292_mut  ZNF423_mut  ZNF521_mut  \\\n0              1           0           0           0           0           0   \n1              0           0           0           0           0           0   \n2              0           0           0           0           0           0   \n3              0           0           0           0           0           0   \n4              0           0           0           0           0           0   \n..           ...         ...         ...         ...         ...         ...   \n432            0           0           0           0           0           0   \n433            0           0           0           0           0           0   \n434            0           0           0           0           0           0   \n435            0           0           0           1           0           0   \n436            0           0           0           0           0           0   \n\n     ZNF536_mut  ZNF626_mut  ZNF804A_mut  ZNF91_mut  ZZEF1_mut  RAS_mut  \n0             0           0            0          0          0        1  \n1             0           0            0          0          0        0  \n2             0           0            0          0          0        0  \n3             0           0            0          0          0        0  \n4             0           1            0          0          0        0  \n..          ...         ...          ...        ...        ...      ...  \n432           0           0            0          0          0        1  \n433           0           1            0          0          0        1  \n434           0           0            0          0          0        0  \n435           0           0            0          0          0        1  \n436           0           0            0          0          0        0  \n\n[436 rows x 2183 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>age</th>\n      <th>is_female</th>\n      <th>AAK1_rnaseq</th>\n      <th>AATK_rnaseq</th>\n      <th>ABCB1_rnaseq</th>\n      <th>ABCG2_rnaseq</th>\n      <th>ABI1_rnaseq</th>\n      <th>ABL1_rnaseq</th>\n      <th>ABL2_rnaseq</th>\n      <th>ACE_rnaseq</th>\n      <th>ACKR1_rnaseq</th>\n      <th>ACKR3_rnaseq</th>\n      <th>ACSL3_rnaseq</th>\n      <th>ACSL6_rnaseq</th>\n      <th>ACVR1B_rnaseq</th>\n      <th>ACVR1C_rnaseq</th>\n      <th>ACVR1_rnaseq</th>\n      <th>ACVR2A_rnaseq</th>\n      <th>ACVR2B_rnaseq</th>\n      <th>ACVRL1_rnaseq</th>\n      <th>ADAM10_rnaseq</th>\n      <th>ADAM17_rnaseq</th>\n      <th>ADCK1_rnaseq</th>\n      <th>ADCK2_rnaseq</th>\n      <th>ADCK5_rnaseq</th>\n      <th>...</th>\n      <th>UTRN_mut</th>\n      <th>VCAN_mut</th>\n      <th>VPS13B_mut</th>\n      <th>VPS13C_mut</th>\n      <th>VPS13D_mut</th>\n      <th>WDFY3_mut</th>\n      <th>WNK1_mut</th>\n      <th>XIRP2_mut</th>\n      <th>XIST_mut</th>\n      <th>ZDBF2_mut</th>\n      <th>ZFHX3_mut</th>\n      <th>ZFHX4_mut</th>\n      <th>ZFP36L1_mut</th>\n      <th>ZFYVE26_mut</th>\n      <th>ZFYVE9_mut</th>\n      <th>ZNF236_mut</th>\n      <th>ZNF292_mut</th>\n      <th>ZNF423_mut</th>\n      <th>ZNF521_mut</th>\n      <th>ZNF536_mut</th>\n      <th>ZNF626_mut</th>\n      <th>ZNF804A_mut</th>\n      <th>ZNF91_mut</th>\n      <th>ZZEF1_mut</th>\n      <th>RAS_mut</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>63</td>\n      <td>0</td>\n      <td>-0.6734</td>\n      <td>-0.4660</td>\n      <td>0.8401</td>\n      <td>-0.2222</td>\n      <td>2.2318</td>\n      <td>-0.8171</td>\n      <td>0.8051</td>\n      <td>-0.1250</td>\n      <td>-0.2976</td>\n      <td>1.2538</td>\n      <td>-0.3237</td>\n      <td>-0.1429</td>\n      <td>0.5258</td>\n      <td>-0.0748</td>\n      <td>-0.2048</td>\n      <td>-0.3004</td>\n      <td>0.2998</td>\n      <td>-0.6414</td>\n      <td>1.2149</td>\n      <td>1.1643</td>\n      <td>0.3720</td>\n      <td>-0.2883</td>\n      <td>-0.1974</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>66</td>\n      <td>0</td>\n      <td>2.4277</td>\n      <td>-0.3853</td>\n      <td>0.1104</td>\n      <td>-0.2183</td>\n      <td>-0.0952</td>\n      <td>-0.6255</td>\n      <td>0.0970</td>\n      <td>-0.4911</td>\n      <td>-0.1779</td>\n      <td>-0.4134</td>\n      <td>-0.1501</td>\n      <td>-0.1576</td>\n      <td>-0.3597</td>\n      <td>0.4555</td>\n      <td>-1.0758</td>\n      <td>0.3252</td>\n      <td>1.7109</td>\n      <td>-0.5763</td>\n      <td>2.5860</td>\n      <td>1.5608</td>\n      <td>-0.6966</td>\n      <td>0.1801</td>\n      <td>-0.3164</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>66</td>\n      <td>0</td>\n      <td>2.4277</td>\n      <td>-0.3853</td>\n      <td>0.1104</td>\n      <td>-0.2183</td>\n      <td>-0.0952</td>\n      <td>-0.6255</td>\n      <td>0.0970</td>\n      <td>-0.4911</td>\n      <td>-0.1779</td>\n      <td>-0.4134</td>\n      <td>-0.1501</td>\n      <td>-0.1576</td>\n      <td>-0.3597</td>\n      <td>0.4555</td>\n      <td>-1.0758</td>\n      <td>0.3252</td>\n      <td>1.7109</td>\n      <td>-0.5763</td>\n      <td>2.5860</td>\n      <td>1.5608</td>\n      <td>-0.6966</td>\n      <td>0.1801</td>\n      <td>-0.3164</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>69</td>\n      <td>0</td>\n      <td>1.1340</td>\n      <td>-0.4110</td>\n      <td>0.1572</td>\n      <td>0.0752</td>\n      <td>0.0566</td>\n      <td>-1.3448</td>\n      <td>-0.3876</td>\n      <td>1.0335</td>\n      <td>-0.3683</td>\n      <td>-0.3736</td>\n      <td>-0.3294</td>\n      <td>-0.1807</td>\n      <td>0.8215</td>\n      <td>-0.7729</td>\n      <td>-0.7901</td>\n      <td>-0.9142</td>\n      <td>-0.2716</td>\n      <td>-0.2526</td>\n      <td>1.2477</td>\n      <td>0.8202</td>\n      <td>-0.1294</td>\n      <td>0.7846</td>\n      <td>-0.2564</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>59</td>\n      <td>1</td>\n      <td>-0.5311</td>\n      <td>0.1418</td>\n      <td>-0.0998</td>\n      <td>-0.2493</td>\n      <td>-0.6956</td>\n      <td>-0.3696</td>\n      <td>-0.1672</td>\n      <td>-0.7257</td>\n      <td>-0.3450</td>\n      <td>-0.1465</td>\n      <td>0.2727</td>\n      <td>0.3077</td>\n      <td>1.3352</td>\n      <td>2.3315</td>\n      <td>0.2386</td>\n      <td>1.6382</td>\n      <td>-0.2124</td>\n      <td>-0.7441</td>\n      <td>0.9661</td>\n      <td>0.6493</td>\n      <td>-1.2289</td>\n      <td>-0.0261</td>\n      <td>-0.1046</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>432</th>\n      <td>71</td>\n      <td>0</td>\n      <td>2.8284</td>\n      <td>0.9219</td>\n      <td>-0.4711</td>\n      <td>-0.1561</td>\n      <td>-0.7569</td>\n      <td>-0.0186</td>\n      <td>2.2843</td>\n      <td>1.9383</td>\n      <td>-0.3507</td>\n      <td>-0.4448</td>\n      <td>-0.4321</td>\n      <td>-0.1794</td>\n      <td>-1.0555</td>\n      <td>-0.6594</td>\n      <td>1.4145</td>\n      <td>-1.0607</td>\n      <td>-0.2349</td>\n      <td>0.9236</td>\n      <td>-0.8023</td>\n      <td>1.6165</td>\n      <td>-1.0423</td>\n      <td>-1.2719</td>\n      <td>-0.7105</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>433</th>\n      <td>61</td>\n      <td>1</td>\n      <td>0.9422</td>\n      <td>0.2662</td>\n      <td>-0.5276</td>\n      <td>-0.3078</td>\n      <td>-0.5164</td>\n      <td>-0.3653</td>\n      <td>1.6644</td>\n      <td>0.4936</td>\n      <td>-0.3722</td>\n      <td>-0.5500</td>\n      <td>-0.3539</td>\n      <td>-0.1324</td>\n      <td>-1.1019</td>\n      <td>-0.7735</td>\n      <td>0.0458</td>\n      <td>-1.1459</td>\n      <td>-0.6342</td>\n      <td>0.4675</td>\n      <td>-1.0416</td>\n      <td>-0.9597</td>\n      <td>-1.3496</td>\n      <td>0.4849</td>\n      <td>1.6007</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>434</th>\n      <td>60</td>\n      <td>1</td>\n      <td>-0.3000</td>\n      <td>-0.5301</td>\n      <td>-0.5559</td>\n      <td>0.4720</td>\n      <td>0.0597</td>\n      <td>-0.1817</td>\n      <td>3.6724</td>\n      <td>0.1842</td>\n      <td>-0.3892</td>\n      <td>-0.6284</td>\n      <td>0.2315</td>\n      <td>-0.1468</td>\n      <td>-1.1972</td>\n      <td>-0.5964</td>\n      <td>-0.2927</td>\n      <td>-0.9101</td>\n      <td>-0.6431</td>\n      <td>-0.3869</td>\n      <td>-0.5002</td>\n      <td>-0.0913</td>\n      <td>-0.6892</td>\n      <td>-0.6854</td>\n      <td>1.1884</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>435</th>\n      <td>62</td>\n      <td>1</td>\n      <td>3.2208</td>\n      <td>-0.2592</td>\n      <td>-0.8130</td>\n      <td>-0.1423</td>\n      <td>-1.2421</td>\n      <td>-1.4423</td>\n      <td>-0.6631</td>\n      <td>-0.9650</td>\n      <td>-0.3868</td>\n      <td>1.9972</td>\n      <td>0.0008</td>\n      <td>-0.1583</td>\n      <td>-1.4766</td>\n      <td>-0.7790</td>\n      <td>-1.2992</td>\n      <td>-1.0639</td>\n      <td>0.5145</td>\n      <td>-0.8358</td>\n      <td>-0.1595</td>\n      <td>-0.0378</td>\n      <td>-0.3054</td>\n      <td>0.7819</td>\n      <td>1.9629</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>436</th>\n      <td>65</td>\n      <td>0</td>\n      <td>-0.3658</td>\n      <td>-0.2651</td>\n      <td>1.5479</td>\n      <td>-0.2854</td>\n      <td>-1.0743</td>\n      <td>3.2052</td>\n      <td>0.2885</td>\n      <td>-0.3852</td>\n      <td>2.6972</td>\n      <td>0.7650</td>\n      <td>-0.7979</td>\n      <td>-0.1453</td>\n      <td>-0.9347</td>\n      <td>-0.5309</td>\n      <td>0.6657</td>\n      <td>-0.2331</td>\n      <td>-0.7919</td>\n      <td>0.7327</td>\n      <td>-0.7508</td>\n      <td>-0.2766</td>\n      <td>-0.1653</td>\n      <td>-0.3842</td>\n      <td>-0.4158</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>436 rows × 2183 columns</p>\n</div>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blca.features\n",
    "\n",
    "# types of features\n",
    "# continuous: *_rnaseq, age\n",
    "# categorical: *_mut, *_cnv "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-27T14:51:49.133839Z",
     "start_time": "2023-10-27T14:51:48.972577Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Flatten(start_dim=1, end_dim=-1)\n",
      "  (1): Linear(in_features=8192, out_features=256, bias=True)\n",
      "  (2): LeakyReLU(negative_slope=0.01)\n",
      "  (3): Dropout(p=0.5, inplace=False)\n",
      "  (4): Linear(in_features=256, out_features=512, bias=True)\n",
      "  (5): LeakyReLU(negative_slope=0.01)\n",
      "  (6): Dropout(p=0.5, inplace=False)\n",
      "  (7): Linear(in_features=512, out_features=1024, bias=True)\n",
      "  (8): LeakyReLU(negative_slope=0.01)\n",
      "  (9): Dropout(p=0.5, inplace=False)\n",
      "  (10): Linear(in_features=1024, out_features=2183, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8431, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.5584, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.7047, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.8956, grad_fn=<MseLossBackward0>)\n",
      "tensor(5.7764, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:07<01:09,  7.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7.2884, grad_fn=<MseLossBackward0>)\n",
      "tensor(20.0209, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.6867, grad_fn=<MseLossBackward0>)\n",
      "tensor(2.2701, grad_fn=<MseLossBackward0>)\n",
      "tensor(1364.9028, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:15<01:00,  7.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1064.7476, grad_fn=<MseLossBackward0>)\n",
      "tensor(84.7511, grad_fn=<MseLossBackward0>)\n",
      "tensor(36.5887, grad_fn=<MseLossBackward0>)\n",
      "tensor(35.9273, grad_fn=<MseLossBackward0>)\n",
      "tensor(40.3912, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:22<00:53,  7.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(38.5193, grad_fn=<MseLossBackward0>)\n",
      "tensor(34.1691, grad_fn=<MseLossBackward0>)\n",
      "tensor(46.3456, grad_fn=<MseLossBackward0>)\n",
      "tensor(27.7616, grad_fn=<MseLossBackward0>)\n",
      "tensor(42.9793, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:30<00:46,  7.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(23.9020, grad_fn=<MseLossBackward0>)\n",
      "tensor(955563.5625, grad_fn=<MseLossBackward0>)\n",
      "tensor(5759.6733, grad_fn=<MseLossBackward0>)\n",
      "tensor(8378.5557, grad_fn=<MseLossBackward0>)\n",
      "tensor(2826.3430, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [00:38<00:38,  7.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3253.9719, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "\n",
    "\n",
    "def pretrain_loop(\n",
    "        data: TCGADataset,\n",
    "        batch_size: int, \n",
    "    ):\n",
    "    \n",
    "    loader = DataLoader(\n",
    "        data, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True, \n",
    "        num_workers=multiprocessing.cpu_count()-1\n",
    "    )\n",
    "    [omic_sample], _, _, _ = next(iter(loader))   \n",
    "    \n",
    "    \n",
    "    model = TabPretrainer(\n",
    "        sample = omic_sample,\n",
    "        # input_channels=omic_sample.shape[-1], \n",
    "        input_axis=1, \n",
    "        latent_shape=[256, 32], \n",
    "        attn_dropout=0.1, \n",
    "        num_heads=4,\n",
    "        num_freq_bands=8\n",
    "    )\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    \n",
    "    loss_method = \"mse\"\n",
    "    loss_fn = TabularLoss(method=loss_method)\n",
    "    \n",
    "    for epoch in tqdm(range(10)):\n",
    "        for idx, batch in enumerate(loader):\n",
    "            [omic], censorship, event_time, y_disc = batch\n",
    "            rec_omic = model(omic)\n",
    "            # print(omic.shape)\n",
    "            if loss_method == \"contrastive\":\n",
    "                # need to pass in larges for contrastive loss\n",
    "                # using torch.ones to ensure that omic and rec_omic are learned as similar representations\n",
    "                # note that this is a slight repurposing of the contrastive loss function\n",
    "                # with this, the loss is just 1-cos(omic, rec_omic)\n",
    "                loss = loss_fn(input1=omic, input2=rec_omic, target=torch.ones(omic.shape[0]))\n",
    "            elif loss_method == \"mse\": \n",
    "                loss = loss_fn(input=omic, target=rec_omic)\n",
    "            # loss = loss_fn(omic, rec_omic)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # print every 10th batch\n",
    "            if idx % 100 == 0:\n",
    "                print(loss)\n",
    "                # print(omic)\n",
    "                # print(rec_omic)\n",
    "            \n",
    "    \n",
    "pretrain_loop(\n",
    "    data=blca, \n",
    "    batch_size=1)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-10-27T15:38:31.871902Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-27T14:53:49.991436Z",
     "start_time": "2023-10-27T14:53:49.862536Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-27T14:53:50.007237Z",
     "start_time": "2023-10-27T14:53:49.867903Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-27T14:53:50.008590Z",
     "start_time": "2023-10-27T14:53:49.868624Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cognition",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
