{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T09:47:43.353798Z",
     "start_time": "2023-10-17T09:47:40.746357Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>.container { width:100% !important; }</style>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "if \"x_perceiver\" not in os.listdir():\n",
    "    os.chdir(\"/home/kh701/pycharm/healnet/\")\n",
    "import torch\n",
    "from torch import nn\n",
    "import multiprocessing\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "import einops\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from healnet.models.explainer import Explainer\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "\n",
    "from healnet.utils import Config, flatten_config\n",
    "from healnet.etl import TCGADataset\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "    \n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T09:47:51.774621Z",
     "start_time": "2023-10-17T09:47:43.134571Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filled 0 missing values with mean\n",
      "Missing values per feature: \n",
      " Series([], dtype: int64)\n",
      "Slides available: 436\n",
      "Omic available: 437\n",
      "Overlap: 436\n",
      "Filtering out 1 samples for which there are no omic data available\n",
      "Dataloader initialised for blca dataset\n",
      "Dataset: BLCA\n",
      "Molecular data shape: (436, 2191)\n",
      "Molecular/Slide match: 436/436\n",
      "Slide level count: 4\n",
      "Slide level dimensions: ((79968, 79653), (19992, 19913), (4998, 4978), (2499, 2489))\n",
      "Slide resize dimensions: w: 1024, h: 1024\n",
      "Sources selected: ['omic']\n",
      "Censored share: 0.539\n",
      "Survival_bin_sizes: {0: 72, 1: 83, 2: 109, 3: 172}\n",
      "Filled 0 missing values with mean\n",
      "Missing values per feature: \n",
      " Series([], dtype: int64)\n",
      "Slides available: 1019\n",
      "Omic available: 1022\n",
      "Overlap: 1019\n",
      "Filtering out 3 samples for which there are no omic data available\n",
      "Dataloader initialised for brca dataset\n",
      "Dataset: BRCA\n",
      "Molecular data shape: (1019, 2922)\n",
      "Molecular/Slide match: 1019/1019\n",
      "Slide level count: 3\n",
      "Slide level dimensions: ((35855, 34985), (8963, 8746), (2240, 2186))\n",
      "Slide resize dimensions: w: 1024, h: 1024\n",
      "Sources selected: ['omic']\n",
      "Censored share: 0.868\n",
      "Survival_bin_sizes: {3: 155, 2: 172, 1: 289, 0: 403}\n"
     ]
    }
   ],
   "source": [
    "# get dataloaders\n",
    "config = Config(\"config/main_gpu.yml\").read()\n",
    "config = flatten_config(config) # TODO - refactor to other \n",
    "\n",
    "blca = TCGADataset(\n",
    "    dataset=\"blca\", \n",
    "    config=config, \n",
    "    level=2, \n",
    "    sources=[\"omic\"]\n",
    ")\n",
    "\n",
    "brca = TCGADataset(\n",
    "    dataset=\"brca\", \n",
    "    config=config, \n",
    "    level=2, \n",
    "    sources=[\"omic\"]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [],
   "source": [
    "# get tabular data\n",
    "blca_loader = DataLoader(\n",
    "    blca, \n",
    "    batch_size=1, \n",
    "    shuffle=True, \n",
    "    num_workers=multiprocessing.cpu_count()-1\n",
    ")\n",
    "[sample], censorship, event_time, y_disc = next(iter(blca_loader))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-17T13:46:40.440300Z",
     "start_time": "2023-10-17T13:46:39.958253Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 1, 2183])"
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-17T13:46:42.257122Z",
     "start_time": "2023-10-17T13:46:42.216297Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tabular self-supervised pre-training\n",
    "\n",
    "To start with, we want to build and encoder-decoder model which trains a cross-attention unit as the encoder, which can later on be deployed in the iterative model. We then want to benchmark the performance with pan-cancer pre-training vs. without pre-training. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [],
   "source": [
    "from healnet.models.healnet import Attention, PreNorm\n",
    "\n",
    "class AttentionEncoder(nn.Module): \n",
    "    \"\"\"\n",
    "    Simple encoder that uses fourier encoding, pre-norm and cross-attention to encode the input features into a latent array \n",
    "    of size (num_latents x latent_dim). Takes in both the input tensors as well as a randomly initialised latent \n",
    "    array as the input. \n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 input_channels: int,\n",
    "                 latent: torch.Tensor, \n",
    "                 input_axis: int = 1, \n",
    "                 attn_dropout: float = 0.1,\n",
    "                 num_heads: int = 4, \n",
    "                 num_freq_bands: int=8, \n",
    "                 ):    \n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_channels = input_channels\n",
    "        self.input_axis = input_axis\n",
    "        self.attn_dropout = attn_dropout\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        \n",
    "        # fourier_channels = (input_axis * ((num_freq_bands * 2) + 1))\n",
    "        # input_dim = fourier_channels + input_channels\n",
    "        input_dim = input_channels\n",
    "                \n",
    "        latent_dim = latent.shape[-1] # required for PreNorm layer\n",
    "        # simple single attention unit\n",
    "        enc = PreNorm(latent_dim, Attention(latent_dim, input_dim, heads=num_heads, dim_head=num_heads, dropout=attn_dropout), context_dim=input_dim)\n",
    "        \n",
    "        self.layers = nn.ModuleList([enc])\n",
    "        \n",
    "    def forward(self, latent: torch.Tensor, context: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Note: context is the data, x is the latent\n",
    "        Args:\n",
    "            latent: \n",
    "            context: \n",
    "\n",
    "        Returns:\n",
    "\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            latent = layer(x=latent, context=context)\n",
    "        return latent\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-17T13:05:11.266538Z",
     "start_time": "2023-10-17T13:05:11.225897Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The decoder often needs to be different depending on the modality, so let's implement modality-specific decoders while trying to have a relatively general-purpose encoder that we can plug into the pipeline.\n",
    "\n",
    "Note that we may change this later down the line. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T13:05:19.660902Z",
     "start_time": "2023-10-17T13:05:19.617063Z"
    }
   },
   "outputs": [],
   "source": [
    "class TabularDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder suited for tabular data. We use the following: \n",
    "    - Skip connections: faster and more stable training\n",
    "    - Batch normalisation: stabilises the activations and speeds up training\n",
    "    - Activation: Output layer to map back to output dimensions, corresponding to the original data dims\n",
    "    Tries to reconstruct the original input given the latent\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_dim: int, output_dim: int):\n",
    "        super(TabularDecoder, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        hidden_dims = [128, 256] # may refactor as hyperparameter later\n",
    "        \n",
    "        for hidden_dim in hidden_dims: \n",
    "            layers.extend([\n",
    "                nn.Linear(in_features=latent_dim, out_features=hidden_dim), \n",
    "                nn.LeakyReLU(), \n",
    "                # nn.BatchNorm1d(hidden_dim, track_running_stats=False), \n",
    "                nn.Dropout(0.5)\n",
    "            ])\n",
    "            \n",
    "            latent_dim = hidden_dim # update for next layer\n",
    "        \n",
    "        # final layer to reconstruct output\n",
    "        layers.append(nn.Linear(latent_dim, output_dim))\n",
    "        \n",
    "        self.decode = nn.Sequential(*layers)\n",
    "        print(self.decode)\n",
    "        \n",
    "    def forward(self, latent: torch.Tensor):\n",
    "        return self.decode(latent)\n",
    "    \n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, putting it all together in the encoder-decoder model\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.float32"
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-17T13:47:02.949274Z",
     "start_time": "2023-10-17T13:47:02.916694Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "outputs": [],
   "source": [
    "from typing import *\n",
    "\n",
    "class TabPretrainer(nn.Module): \n",
    "    \"\"\"\n",
    "    Encoder-decoder model for pre-training tabular data.\n",
    "    # TODO - refactor abstract base class for initialisations \n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 sample: torch.Tensor,\n",
    "                 # input_channels: int,\n",
    "                 latent_shape: List[int],\n",
    "                 input_axis: int = 1,\n",
    "                 attn_dropout: float = 0.1,\n",
    "                 num_heads: int = 4,\n",
    "                 num_freq_bands: int=8,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.input_channels = sample.shape[-1]\n",
    "        self.input_axis = input_axis\n",
    "        self.num_latents, self.latent_dim = latent_shape\n",
    "        self.attn_dropout = attn_dropout\n",
    "        self.num_heads = num_heads\n",
    "        self.num_freq_bands = num_freq_bands\n",
    "        \n",
    "        \n",
    "        # randomly initialise latent\n",
    "        self.latent = nn.Parameter(torch.randn(self.num_latents, self.latent_dim))\n",
    "        \n",
    "        # encoder\n",
    "        self.encoder = AttentionEncoder(\n",
    "            input_channels=self.input_channels, \n",
    "            latent=self.latent, \n",
    "            input_axis=self.input_axis, \n",
    "            attn_dropout=attn_dropout, \n",
    "            num_heads=num_heads, \n",
    "            num_freq_bands=num_freq_bands\n",
    "        )\n",
    "        \n",
    "        # decoder\n",
    "        self.decoder = TabularDecoder(\n",
    "            latent_dim=self.latent_dim, \n",
    "            output_dim=self.input_channels\n",
    "        )\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # get batch dim\n",
    "        b = x.shape[0]\n",
    "        \n",
    "        # expand latent to batch size\n",
    "        if len(self.latent.shape) == 2:\n",
    "            self.latent = nn.Parameter(einops.repeat(self.latent, \"n d -> b n d\", b=b))\n",
    "        \n",
    "        # encode\n",
    "        self.latent.data = self.encoder(latent=self.latent, context=x).data + self.latent.data\n",
    "        \n",
    "        # decode, reconstructed x\n",
    "        rec_x = self.decoder(self.latent)\n",
    "        return rec_x\n",
    "    \n",
    "    def get_latent(self):\n",
    "        return self.latent"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-17T13:57:49.077669Z",
     "start_time": "2023-10-17T13:57:49.027628Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we need to think about tabular loss functions. Here, we can explore both reconstruction losses and contrastive losses. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "class TabularLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Reconstruction loss functions for tabular data. We use two types which are commonly used with continuous data: \n",
    "    - Mean squared error\n",
    "    - Constrastive loss, measured as cosine distance between the original and reconstructed data\n",
    "    We seek to minimise both objectives.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 method: str = \"mse\",\n",
    "                 reduction: str = \"mean\",\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        assert method in [\"mse\", \"contrastive\"], \"Loss type not recognised\"\n",
    "        self.loss_type = method\n",
    "        self.reduction = reduction\n",
    "        \n",
    "        if method == \"mse\":\n",
    "            self.loss = nn.MSELoss(reduction=reduction)\n",
    "        elif method == \"contrastive\":\n",
    "            self.loss = nn.CosineEmbeddingLoss(reduction=reduction)\n",
    "            \n",
    "    def __call__(self, x: torch.Tensor, rec_x: torch.Tensor):\n",
    "        return self.loss(x, rec_x)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-17T09:47:52.379618Z",
     "start_time": "2023-10-17T09:47:52.309152Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, we write a pre-training loop that we can use for pre-training across cancer sites. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2191 2922 1758\n"
     ]
    }
   ],
   "source": [
    "# get overlap between omic columns\n",
    "col1 = blca.omic_df.columns\n",
    "col2 = brca.omic_df.columns\n",
    "print(len(col1), len(col2), len(set(col1).intersection(col2)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-17T09:47:52.379967Z",
     "start_time": "2023-10-17T09:47:52.352488Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [
    {
     "data": {
      "text/plain": "     age  is_female  AAK1_rnaseq  AATK_rnaseq  ABCB1_rnaseq  ABCG2_rnaseq  \\\n0     63          0      -0.6734      -0.4660        0.8401       -0.2222   \n1     66          0       2.4277      -0.3853        0.1104       -0.2183   \n2     66          0       2.4277      -0.3853        0.1104       -0.2183   \n3     69          0       1.1340      -0.4110        0.1572        0.0752   \n4     59          1      -0.5311       0.1418       -0.0998       -0.2493   \n..   ...        ...          ...          ...           ...           ...   \n432   71          0       2.8284       0.9219       -0.4711       -0.1561   \n433   61          1       0.9422       0.2662       -0.5276       -0.3078   \n434   60          1      -0.3000      -0.5301       -0.5559        0.4720   \n435   62          1       3.2208      -0.2592       -0.8130       -0.1423   \n436   65          0      -0.3658      -0.2651        1.5479       -0.2854   \n\n     ABI1_rnaseq  ABL1_rnaseq  ABL2_rnaseq  ACE_rnaseq  ACKR1_rnaseq  \\\n0         2.2318      -0.8171       0.8051     -0.1250       -0.2976   \n1        -0.0952      -0.6255       0.0970     -0.4911       -0.1779   \n2        -0.0952      -0.6255       0.0970     -0.4911       -0.1779   \n3         0.0566      -1.3448      -0.3876      1.0335       -0.3683   \n4        -0.6956      -0.3696      -0.1672     -0.7257       -0.3450   \n..           ...          ...          ...         ...           ...   \n432      -0.7569      -0.0186       2.2843      1.9383       -0.3507   \n433      -0.5164      -0.3653       1.6644      0.4936       -0.3722   \n434       0.0597      -0.1817       3.6724      0.1842       -0.3892   \n435      -1.2421      -1.4423      -0.6631     -0.9650       -0.3868   \n436      -1.0743       3.2052       0.2885     -0.3852        2.6972   \n\n     ACKR3_rnaseq  ACSL3_rnaseq  ACSL6_rnaseq  ACVR1B_rnaseq  ACVR1C_rnaseq  \\\n0          1.2538       -0.3237       -0.1429         0.5258        -0.0748   \n1         -0.4134       -0.1501       -0.1576        -0.3597         0.4555   \n2         -0.4134       -0.1501       -0.1576        -0.3597         0.4555   \n3         -0.3736       -0.3294       -0.1807         0.8215        -0.7729   \n4         -0.1465        0.2727        0.3077         1.3352         2.3315   \n..            ...           ...           ...            ...            ...   \n432       -0.4448       -0.4321       -0.1794        -1.0555        -0.6594   \n433       -0.5500       -0.3539       -0.1324        -1.1019        -0.7735   \n434       -0.6284        0.2315       -0.1468        -1.1972        -0.5964   \n435        1.9972        0.0008       -0.1583        -1.4766        -0.7790   \n436        0.7650       -0.7979       -0.1453        -0.9347        -0.5309   \n\n     ACVR1_rnaseq  ACVR2A_rnaseq  ACVR2B_rnaseq  ACVRL1_rnaseq  ADAM10_rnaseq  \\\n0         -0.2048        -0.3004         0.2998        -0.6414         1.2149   \n1         -1.0758         0.3252         1.7109        -0.5763         2.5860   \n2         -1.0758         0.3252         1.7109        -0.5763         2.5860   \n3         -0.7901        -0.9142        -0.2716        -0.2526         1.2477   \n4          0.2386         1.6382        -0.2124        -0.7441         0.9661   \n..            ...            ...            ...            ...            ...   \n432        1.4145        -1.0607        -0.2349         0.9236        -0.8023   \n433        0.0458        -1.1459        -0.6342         0.4675        -1.0416   \n434       -0.2927        -0.9101        -0.6431        -0.3869        -0.5002   \n435       -1.2992        -1.0639         0.5145        -0.8358        -0.1595   \n436        0.6657        -0.2331        -0.7919         0.7327        -0.7508   \n\n     ADAM17_rnaseq  ADCK1_rnaseq  ADCK2_rnaseq  ADCK5_rnaseq  ...  UTRN_mut  \\\n0           1.1643        0.3720       -0.2883       -0.1974  ...         1   \n1           1.5608       -0.6966        0.1801       -0.3164  ...         0   \n2           1.5608       -0.6966        0.1801       -0.3164  ...         0   \n3           0.8202       -0.1294        0.7846       -0.2564  ...         0   \n4           0.6493       -1.2289       -0.0261       -0.1046  ...         0   \n..             ...           ...           ...           ...  ...       ...   \n432         1.6165       -1.0423       -1.2719       -0.7105  ...         0   \n433        -0.9597       -1.3496        0.4849        1.6007  ...         0   \n434        -0.0913       -0.6892       -0.6854        1.1884  ...         0   \n435        -0.0378       -0.3054        0.7819        1.9629  ...         0   \n436        -0.2766       -0.1653       -0.3842       -0.4158  ...         0   \n\n     VCAN_mut  VPS13B_mut  VPS13C_mut  VPS13D_mut  WDFY3_mut  WNK1_mut  \\\n0           0           0           0           0          0         0   \n1           0           0           0           0          0         0   \n2           0           0           0           0          0         0   \n3           0           0           0           0          0         0   \n4           0           0           0           0          0         0   \n..        ...         ...         ...         ...        ...       ...   \n432         0           0           1           0          0         0   \n433         0           0           0           0          0         0   \n434         0           0           0           0          0         0   \n435         0           0           0           0          0         0   \n436         0           0           0           0          0         0   \n\n     XIRP2_mut  XIST_mut  ZDBF2_mut  ZFHX3_mut  ZFHX4_mut  ZFP36L1_mut  \\\n0            0         0          0          0          1            0   \n1            0         0          0          0          1            0   \n2            0         0          0          0          1            0   \n3            0         0          0          0          0            0   \n4            0         1          0          1          1            0   \n..         ...       ...        ...        ...        ...          ...   \n432          0         0          0          0          0            0   \n433          1         0          0          0          1            0   \n434          0         0          0          0          0            0   \n435          0         0          0          0          0            1   \n436          0         0          0          0          0            0   \n\n     ZFYVE26_mut  ZFYVE9_mut  ZNF236_mut  ZNF292_mut  ZNF423_mut  ZNF521_mut  \\\n0              1           0           0           0           0           0   \n1              0           0           0           0           0           0   \n2              0           0           0           0           0           0   \n3              0           0           0           0           0           0   \n4              0           0           0           0           0           0   \n..           ...         ...         ...         ...         ...         ...   \n432            0           0           0           0           0           0   \n433            0           0           0           0           0           0   \n434            0           0           0           0           0           0   \n435            0           0           0           1           0           0   \n436            0           0           0           0           0           0   \n\n     ZNF536_mut  ZNF626_mut  ZNF804A_mut  ZNF91_mut  ZZEF1_mut  RAS_mut  \n0             0           0            0          0          0        1  \n1             0           0            0          0          0        0  \n2             0           0            0          0          0        0  \n3             0           0            0          0          0        0  \n4             0           1            0          0          0        0  \n..          ...         ...          ...        ...        ...      ...  \n432           0           0            0          0          0        1  \n433           0           1            0          0          0        1  \n434           0           0            0          0          0        0  \n435           0           0            0          0          0        1  \n436           0           0            0          0          0        0  \n\n[436 rows x 2183 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>age</th>\n      <th>is_female</th>\n      <th>AAK1_rnaseq</th>\n      <th>AATK_rnaseq</th>\n      <th>ABCB1_rnaseq</th>\n      <th>ABCG2_rnaseq</th>\n      <th>ABI1_rnaseq</th>\n      <th>ABL1_rnaseq</th>\n      <th>ABL2_rnaseq</th>\n      <th>ACE_rnaseq</th>\n      <th>ACKR1_rnaseq</th>\n      <th>ACKR3_rnaseq</th>\n      <th>ACSL3_rnaseq</th>\n      <th>ACSL6_rnaseq</th>\n      <th>ACVR1B_rnaseq</th>\n      <th>ACVR1C_rnaseq</th>\n      <th>ACVR1_rnaseq</th>\n      <th>ACVR2A_rnaseq</th>\n      <th>ACVR2B_rnaseq</th>\n      <th>ACVRL1_rnaseq</th>\n      <th>ADAM10_rnaseq</th>\n      <th>ADAM17_rnaseq</th>\n      <th>ADCK1_rnaseq</th>\n      <th>ADCK2_rnaseq</th>\n      <th>ADCK5_rnaseq</th>\n      <th>...</th>\n      <th>UTRN_mut</th>\n      <th>VCAN_mut</th>\n      <th>VPS13B_mut</th>\n      <th>VPS13C_mut</th>\n      <th>VPS13D_mut</th>\n      <th>WDFY3_mut</th>\n      <th>WNK1_mut</th>\n      <th>XIRP2_mut</th>\n      <th>XIST_mut</th>\n      <th>ZDBF2_mut</th>\n      <th>ZFHX3_mut</th>\n      <th>ZFHX4_mut</th>\n      <th>ZFP36L1_mut</th>\n      <th>ZFYVE26_mut</th>\n      <th>ZFYVE9_mut</th>\n      <th>ZNF236_mut</th>\n      <th>ZNF292_mut</th>\n      <th>ZNF423_mut</th>\n      <th>ZNF521_mut</th>\n      <th>ZNF536_mut</th>\n      <th>ZNF626_mut</th>\n      <th>ZNF804A_mut</th>\n      <th>ZNF91_mut</th>\n      <th>ZZEF1_mut</th>\n      <th>RAS_mut</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>63</td>\n      <td>0</td>\n      <td>-0.6734</td>\n      <td>-0.4660</td>\n      <td>0.8401</td>\n      <td>-0.2222</td>\n      <td>2.2318</td>\n      <td>-0.8171</td>\n      <td>0.8051</td>\n      <td>-0.1250</td>\n      <td>-0.2976</td>\n      <td>1.2538</td>\n      <td>-0.3237</td>\n      <td>-0.1429</td>\n      <td>0.5258</td>\n      <td>-0.0748</td>\n      <td>-0.2048</td>\n      <td>-0.3004</td>\n      <td>0.2998</td>\n      <td>-0.6414</td>\n      <td>1.2149</td>\n      <td>1.1643</td>\n      <td>0.3720</td>\n      <td>-0.2883</td>\n      <td>-0.1974</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>66</td>\n      <td>0</td>\n      <td>2.4277</td>\n      <td>-0.3853</td>\n      <td>0.1104</td>\n      <td>-0.2183</td>\n      <td>-0.0952</td>\n      <td>-0.6255</td>\n      <td>0.0970</td>\n      <td>-0.4911</td>\n      <td>-0.1779</td>\n      <td>-0.4134</td>\n      <td>-0.1501</td>\n      <td>-0.1576</td>\n      <td>-0.3597</td>\n      <td>0.4555</td>\n      <td>-1.0758</td>\n      <td>0.3252</td>\n      <td>1.7109</td>\n      <td>-0.5763</td>\n      <td>2.5860</td>\n      <td>1.5608</td>\n      <td>-0.6966</td>\n      <td>0.1801</td>\n      <td>-0.3164</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>66</td>\n      <td>0</td>\n      <td>2.4277</td>\n      <td>-0.3853</td>\n      <td>0.1104</td>\n      <td>-0.2183</td>\n      <td>-0.0952</td>\n      <td>-0.6255</td>\n      <td>0.0970</td>\n      <td>-0.4911</td>\n      <td>-0.1779</td>\n      <td>-0.4134</td>\n      <td>-0.1501</td>\n      <td>-0.1576</td>\n      <td>-0.3597</td>\n      <td>0.4555</td>\n      <td>-1.0758</td>\n      <td>0.3252</td>\n      <td>1.7109</td>\n      <td>-0.5763</td>\n      <td>2.5860</td>\n      <td>1.5608</td>\n      <td>-0.6966</td>\n      <td>0.1801</td>\n      <td>-0.3164</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>69</td>\n      <td>0</td>\n      <td>1.1340</td>\n      <td>-0.4110</td>\n      <td>0.1572</td>\n      <td>0.0752</td>\n      <td>0.0566</td>\n      <td>-1.3448</td>\n      <td>-0.3876</td>\n      <td>1.0335</td>\n      <td>-0.3683</td>\n      <td>-0.3736</td>\n      <td>-0.3294</td>\n      <td>-0.1807</td>\n      <td>0.8215</td>\n      <td>-0.7729</td>\n      <td>-0.7901</td>\n      <td>-0.9142</td>\n      <td>-0.2716</td>\n      <td>-0.2526</td>\n      <td>1.2477</td>\n      <td>0.8202</td>\n      <td>-0.1294</td>\n      <td>0.7846</td>\n      <td>-0.2564</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>59</td>\n      <td>1</td>\n      <td>-0.5311</td>\n      <td>0.1418</td>\n      <td>-0.0998</td>\n      <td>-0.2493</td>\n      <td>-0.6956</td>\n      <td>-0.3696</td>\n      <td>-0.1672</td>\n      <td>-0.7257</td>\n      <td>-0.3450</td>\n      <td>-0.1465</td>\n      <td>0.2727</td>\n      <td>0.3077</td>\n      <td>1.3352</td>\n      <td>2.3315</td>\n      <td>0.2386</td>\n      <td>1.6382</td>\n      <td>-0.2124</td>\n      <td>-0.7441</td>\n      <td>0.9661</td>\n      <td>0.6493</td>\n      <td>-1.2289</td>\n      <td>-0.0261</td>\n      <td>-0.1046</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>432</th>\n      <td>71</td>\n      <td>0</td>\n      <td>2.8284</td>\n      <td>0.9219</td>\n      <td>-0.4711</td>\n      <td>-0.1561</td>\n      <td>-0.7569</td>\n      <td>-0.0186</td>\n      <td>2.2843</td>\n      <td>1.9383</td>\n      <td>-0.3507</td>\n      <td>-0.4448</td>\n      <td>-0.4321</td>\n      <td>-0.1794</td>\n      <td>-1.0555</td>\n      <td>-0.6594</td>\n      <td>1.4145</td>\n      <td>-1.0607</td>\n      <td>-0.2349</td>\n      <td>0.9236</td>\n      <td>-0.8023</td>\n      <td>1.6165</td>\n      <td>-1.0423</td>\n      <td>-1.2719</td>\n      <td>-0.7105</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>433</th>\n      <td>61</td>\n      <td>1</td>\n      <td>0.9422</td>\n      <td>0.2662</td>\n      <td>-0.5276</td>\n      <td>-0.3078</td>\n      <td>-0.5164</td>\n      <td>-0.3653</td>\n      <td>1.6644</td>\n      <td>0.4936</td>\n      <td>-0.3722</td>\n      <td>-0.5500</td>\n      <td>-0.3539</td>\n      <td>-0.1324</td>\n      <td>-1.1019</td>\n      <td>-0.7735</td>\n      <td>0.0458</td>\n      <td>-1.1459</td>\n      <td>-0.6342</td>\n      <td>0.4675</td>\n      <td>-1.0416</td>\n      <td>-0.9597</td>\n      <td>-1.3496</td>\n      <td>0.4849</td>\n      <td>1.6007</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>434</th>\n      <td>60</td>\n      <td>1</td>\n      <td>-0.3000</td>\n      <td>-0.5301</td>\n      <td>-0.5559</td>\n      <td>0.4720</td>\n      <td>0.0597</td>\n      <td>-0.1817</td>\n      <td>3.6724</td>\n      <td>0.1842</td>\n      <td>-0.3892</td>\n      <td>-0.6284</td>\n      <td>0.2315</td>\n      <td>-0.1468</td>\n      <td>-1.1972</td>\n      <td>-0.5964</td>\n      <td>-0.2927</td>\n      <td>-0.9101</td>\n      <td>-0.6431</td>\n      <td>-0.3869</td>\n      <td>-0.5002</td>\n      <td>-0.0913</td>\n      <td>-0.6892</td>\n      <td>-0.6854</td>\n      <td>1.1884</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>435</th>\n      <td>62</td>\n      <td>1</td>\n      <td>3.2208</td>\n      <td>-0.2592</td>\n      <td>-0.8130</td>\n      <td>-0.1423</td>\n      <td>-1.2421</td>\n      <td>-1.4423</td>\n      <td>-0.6631</td>\n      <td>-0.9650</td>\n      <td>-0.3868</td>\n      <td>1.9972</td>\n      <td>0.0008</td>\n      <td>-0.1583</td>\n      <td>-1.4766</td>\n      <td>-0.7790</td>\n      <td>-1.2992</td>\n      <td>-1.0639</td>\n      <td>0.5145</td>\n      <td>-0.8358</td>\n      <td>-0.1595</td>\n      <td>-0.0378</td>\n      <td>-0.3054</td>\n      <td>0.7819</td>\n      <td>1.9629</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>436</th>\n      <td>65</td>\n      <td>0</td>\n      <td>-0.3658</td>\n      <td>-0.2651</td>\n      <td>1.5479</td>\n      <td>-0.2854</td>\n      <td>-1.0743</td>\n      <td>3.2052</td>\n      <td>0.2885</td>\n      <td>-0.3852</td>\n      <td>2.6972</td>\n      <td>0.7650</td>\n      <td>-0.7979</td>\n      <td>-0.1453</td>\n      <td>-0.9347</td>\n      <td>-0.5309</td>\n      <td>0.6657</td>\n      <td>-0.2331</td>\n      <td>-0.7919</td>\n      <td>0.7327</td>\n      <td>-0.7508</td>\n      <td>-0.2766</td>\n      <td>-0.1653</td>\n      <td>-0.3842</td>\n      <td>-0.4158</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>436 rows × 2183 columns</p>\n</div>"
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blca.features\n",
    "\n",
    "# types of features\n",
    "# continuous: *_rnaseq, age\n",
    "# categorical: *_mut, *_cnv "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-17T13:23:18.684919Z",
     "start_time": "2023-10-17T13:23:18.619360Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=32, out_features=128, bias=True)\n",
      "  (1): LeakyReLU(negative_slope=0.01)\n",
      "  (2): Dropout(p=0.5, inplace=False)\n",
      "  (3): Linear(in_features=128, out_features=256, bias=True)\n",
      "  (4): LeakyReLU(negative_slope=0.01)\n",
      "  (5): Dropout(p=0.5, inplace=False)\n",
      "  (6): Linear(in_features=256, out_features=2183, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.8114, grad_fn=<MseLossBackward0>)\n",
      "tensor([[[    66.0000,      0.0000,     -0.0004,  ...,      0.0000,\n",
      "               0.0000,      0.0000]]])\n",
      "tensor([[[     0.2862,      0.2241,     -0.1490,  ...,      0.1617,\n",
      "               0.1011,     -0.1182],\n",
      "         [     0.0690,      0.0426,      0.0028,  ...,     -0.0440,\n",
      "               0.0984,     -0.1003],\n",
      "         [    -0.0538,      0.0633,     -0.1094,  ...,     -0.2010,\n",
      "              -0.1546,     -0.0747],\n",
      "         ...,\n",
      "         [    -0.2268,     -0.0646,      0.1491,  ...,     -0.2153,\n",
      "               0.0527,      0.0817],\n",
      "         [    -0.0800,      0.1510,     -0.1117,  ...,     -0.1109,\n",
      "               0.0588,      0.1342],\n",
      "         [    -0.2746,      0.1651,     -0.0003,  ...,     -0.0946,\n",
      "               0.0997,      0.2341]]], grad_fn=<ViewBackward0>)\n",
      "tensor(1.3467, grad_fn=<MseLossBackward0>)\n",
      "tensor([[[50.0000,  1.0000, -1.2060,  ...,  0.0000,  0.0000,  1.0000]]])\n",
      "tensor([[[    27.5842,     -0.2846,      0.1688,  ...,      0.1416,\n",
      "               0.0582,     -0.1577],\n",
      "         [    41.1130,      0.0557,     -0.1878,  ...,     -0.4860,\n",
      "              -0.0484,     -0.4965],\n",
      "         [    28.7888,      0.6149,      0.2294,  ...,      0.0234,\n",
      "               0.2988,      0.0410],\n",
      "         ...,\n",
      "         [    42.1852,      0.1086,     -0.1191,  ...,     -0.4022,\n",
      "               0.5326,      0.1026],\n",
      "         [    32.5511,      0.4902,      0.0678,  ...,     -0.2384,\n",
      "               0.7511,      0.2469],\n",
      "         [    16.7067,     -0.0225,      0.4668,  ...,     -0.1840,\n",
      "               0.2974,     -0.4936]]], grad_fn=<ViewBackward0>)\n",
      "tensor(2.0002, grad_fn=<MseLossBackward0>)\n",
      "tensor([[[57.0000,  1.0000,  0.3662,  ...,  0.0000,  0.0000,  0.0000]]])\n",
      "tensor([[[    63.8325,      0.1519,      0.8597,  ...,      0.1322,\n",
      "               0.0246,      0.3599],\n",
      "         [    50.7889,      0.0764,      0.4523,  ...,      0.1224,\n",
      "              -0.0089,      0.0976],\n",
      "         [    54.3396,      0.2727,      0.4467,  ...,      0.2542,\n",
      "              -0.0584,      0.1118],\n",
      "         ...,\n",
      "         [    47.3579,      0.2425,     -0.4627,  ...,      0.0860,\n",
      "              -0.0025,     -0.0135],\n",
      "         [    42.8474,      0.3923,      0.6955,  ...,      0.1249,\n",
      "              -0.0864,      0.2852],\n",
      "         [    51.2437,      0.3678,     -0.2333,  ...,     -0.0247,\n",
      "              -0.0925,     -0.0696]]], grad_fn=<ViewBackward0>)\n",
      "tensor(1.2329, grad_fn=<MseLossBackward0>)\n",
      "tensor([[[81.0000,  1.0000,  0.1229,  ...,  0.0000,  0.0000,  0.0000]]])\n",
      "tensor([[[    63.3648,      0.5681,      1.1255,  ...,     -0.0677,\n",
      "              -0.0232,      0.0854],\n",
      "         [    38.7918,      0.3735,      0.6946,  ...,     -0.0827,\n",
      "              -0.0160,     -0.0402],\n",
      "         [    71.2584,      0.6003,      0.6359,  ...,     -0.0993,\n",
      "              -0.0397,      0.0868],\n",
      "         ...,\n",
      "         [    42.7554,      0.3118,      0.5133,  ...,     -0.0906,\n",
      "              -0.0118,      0.0614],\n",
      "         [    54.6100,      0.5519,      1.6470,  ...,     -0.0352,\n",
      "              -0.0603,      0.1649],\n",
      "         [    42.5083,      0.4084,      1.0119,  ...,     -0.0571,\n",
      "              -0.0370,      0.1486]]], grad_fn=<ViewBackward0>)\n",
      "tensor(0.6425, grad_fn=<MseLossBackward0>)\n",
      "tensor([[[59.0000,  0.0000, -1.1267,  ...,  1.0000,  0.0000,  0.0000]]])\n",
      "tensor([[[    49.2964,      0.6404,      0.0104,  ...,      0.2961,\n",
      "              -0.0015,      0.0457],\n",
      "         [    75.9104,      0.6590,     -0.5488,  ...,      0.3685,\n",
      "              -0.0775,      0.1171],\n",
      "         [    50.2221,      0.3514,     -0.1757,  ...,      0.2347,\n",
      "               0.0375,      0.0352],\n",
      "         ...,\n",
      "         [    56.9351,      0.5527,     -0.0258,  ...,      0.2873,\n",
      "              -0.0336,      0.0592],\n",
      "         [    76.1894,      0.5842,      0.2018,  ...,      0.3847,\n",
      "               0.0413,      0.1114],\n",
      "         [    70.2016,      0.6631,     -0.7889,  ...,      0.3853,\n",
      "              -0.0584,      0.1157]]], grad_fn=<ViewBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:05<00:47,  5.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.9094, grad_fn=<MseLossBackward0>)\n",
      "tensor([[[80.0000,  0.0000, -1.8057,  ...,  0.0000,  0.0000,  1.0000]]])\n",
      "tensor([[[   70.4145,     0.4202,     0.8595,  ...,     0.1450,\n",
      "              0.0497,     0.1270],\n",
      "         [   46.8503,     0.3871,     0.5777,  ...,     0.1016,\n",
      "              0.0222,     0.1752],\n",
      "         [   49.7496,     0.3898,     0.5145,  ...,     0.1003,\n",
      "              0.0303,     0.1176],\n",
      "         ...,\n",
      "         [   67.4621,     0.5773,     0.7473,  ...,     0.1210,\n",
      "              0.0135,     0.1590],\n",
      "         [   50.6594,     0.3019,     0.3518,  ...,     0.0831,\n",
      "              0.0086,     0.1310],\n",
      "         [   52.9864,     0.4540,     0.6558,  ...,     0.1102,\n",
      "              0.0161,     0.1233]]], grad_fn=<ViewBackward0>)\n",
      "tensor(1.6493, grad_fn=<MseLossBackward0>)\n",
      "tensor([[[46.0000,  0.0000, -0.6910,  ...,  0.0000,  0.0000,  0.0000]]])\n",
      "tensor([[[    60.9179,      0.0647,      1.1114,  ...,     -0.0038,\n",
      "               0.2147,      0.2177],\n",
      "         [    60.0517,      0.2646,      1.3079,  ...,      0.0075,\n",
      "               0.1899,      0.2057],\n",
      "         [    61.6868,      0.1387,      0.5897,  ...,     -0.0201,\n",
      "               0.1978,      0.2022],\n",
      "         ...,\n",
      "         [    68.0292,      0.0506,      1.7228,  ...,      0.0647,\n",
      "               0.2608,      0.2643],\n",
      "         [    59.2711,      0.1455,      0.7053,  ...,      0.0397,\n",
      "               0.1888,      0.2663],\n",
      "         [    48.1980,      0.0181,      0.7638,  ...,      0.0198,\n",
      "               0.1276,      0.1563]]], grad_fn=<ViewBackward0>)\n",
      "tensor(0.7205, grad_fn=<MseLossBackward0>)\n",
      "tensor([[[52.0000,  0.0000,  1.7247,  ...,  0.0000,  0.0000,  1.0000]]])\n",
      "tensor([[[    55.9798,      0.4212,      0.3515,  ...,      0.0235,\n",
      "               0.0190,     -0.1847],\n",
      "         [    54.2648,      0.4556,      0.6894,  ...,     -0.0012,\n",
      "               0.0296,     -0.1335],\n",
      "         [    54.8968,      0.4165,      0.3958,  ...,      0.0351,\n",
      "               0.0403,     -0.1778],\n",
      "         ...,\n",
      "         [    76.4689,      0.6310,      0.8137,  ...,      0.0545,\n",
      "               0.0293,     -0.2406],\n",
      "         [    53.3154,      0.4383,      0.4354,  ...,      0.0250,\n",
      "               0.0102,     -0.1634],\n",
      "         [    70.6945,      0.5075,      0.6261,  ...,      0.0481,\n",
      "               0.0380,     -0.2512]]], grad_fn=<ViewBackward0>)\n",
      "tensor(1.3409, grad_fn=<MseLossBackward0>)\n",
      "tensor([[[77.0000,  0.0000,  2.3937,  ...,  0.0000,  0.0000,  0.0000]]])\n",
      "tensor([[[70.0363,  0.6089,  0.1749,  ..., -0.1550,  0.1805,  0.1468],\n",
      "         [65.2861,  0.5921,  0.3393,  ..., -0.1188,  0.1590,  0.2038],\n",
      "         [56.8061,  0.5692,  0.5429,  ..., -0.1248,  0.1390,  0.1749],\n",
      "         ...,\n",
      "         [62.9669,  0.5928,  0.4240,  ..., -0.1281,  0.1994,  0.2072],\n",
      "         [72.5460,  0.7337,  0.2061,  ..., -0.1197,  0.2218,  0.1867],\n",
      "         [60.7166,  0.5717,  0.4140,  ..., -0.1572,  0.2043,  0.1313]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "tensor(2.1249, grad_fn=<MseLossBackward0>)\n",
      "tensor([[[83.0000,  0.0000,  4.0625,  ...,  0.0000,  1.0000,  0.0000]]])\n",
      "tensor([[[50.9382,  0.2009,  0.2954,  ...,  0.1638,  0.1624,  0.2934],\n",
      "         [63.8772,  0.2047,  0.2163,  ...,  0.2080,  0.1780,  0.3264],\n",
      "         [40.8275,  0.1247,  0.1796,  ...,  0.1448,  0.1023,  0.2185],\n",
      "         ...,\n",
      "         [61.9272,  0.2191,  0.2213,  ...,  0.1709,  0.1618,  0.3108],\n",
      "         [42.8124,  0.1095,  0.2101,  ...,  0.1262,  0.0957,  0.2380],\n",
      "         [64.8580,  0.1948,  0.0649,  ...,  0.2061,  0.1596,  0.3324]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:12<00:50,  6.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7872, grad_fn=<MseLossBackward0>)\n",
      "tensor([[[53.0000,  1.0000,  1.3446,  ...,  0.0000,  0.0000,  0.0000]]])\n",
      "tensor([[[    52.6728,      0.1659,     -0.0653,  ...,      0.3115,\n",
      "               0.1767,      0.1223],\n",
      "         [    71.6219,      0.2006,     -0.0610,  ...,      0.3982,\n",
      "               0.2194,      0.1343],\n",
      "         [    43.1795,      0.1165,     -0.0258,  ...,      0.2429,\n",
      "               0.1552,      0.1066],\n",
      "         ...,\n",
      "         [    57.4983,      0.1863,     -0.1262,  ...,      0.3237,\n",
      "               0.1927,      0.1456],\n",
      "         [    46.9717,      0.1378,      0.0604,  ...,      0.3120,\n",
      "               0.1601,      0.1390],\n",
      "         [    37.7055,      0.1338,     -0.0741,  ...,      0.1984,\n",
      "               0.1208,      0.1106]]], grad_fn=<ViewBackward0>)\n",
      "tensor(1.3723, grad_fn=<MseLossBackward0>)\n",
      "tensor([[[60.0000,  0.0000, -0.4841,  ...,  0.0000,  1.0000,  0.0000]]])\n",
      "tensor([[[    48.0910,      0.3337,      0.0040,  ...,      0.2478,\n",
      "               0.2922,     -0.0654],\n",
      "         [    59.9834,      0.3615,      0.1103,  ...,      0.2671,\n",
      "               0.3670,     -0.0698],\n",
      "         [    60.8409,      0.3197,      0.2644,  ...,      0.2447,\n",
      "               0.3206,     -0.0659],\n",
      "         ...,\n",
      "         [    43.9563,      0.2752,      0.2100,  ...,      0.1779,\n",
      "               0.2552,     -0.0149],\n",
      "         [    76.9193,      0.4720,      0.1428,  ...,      0.3501,\n",
      "               0.4331,     -0.1059],\n",
      "         [    67.3626,      0.4225,      0.2495,  ...,      0.2830,\n",
      "               0.3907,     -0.0768]]], grad_fn=<ViewBackward0>)\n",
      "tensor(1.1830, grad_fn=<MseLossBackward0>)\n",
      "tensor([[[67.0000,  0.0000, -0.4402,  ...,  0.0000,  0.0000,  0.0000]]])\n",
      "tensor([[[    49.4242,     -0.0174,      0.3204,  ...,      0.2927,\n",
      "               0.0434,      0.1772],\n",
      "         [    66.5416,      0.0264,      0.3411,  ...,      0.4118,\n",
      "               0.0703,      0.2081],\n",
      "         [    53.1522,     -0.0116,      0.2520,  ...,      0.3183,\n",
      "               0.0282,      0.2079],\n",
      "         ...,\n",
      "         [    52.8523,      0.0009,      0.3239,  ...,      0.3330,\n",
      "               0.0594,      0.1644],\n",
      "         [    67.9499,     -0.0101,      0.2823,  ...,      0.3738,\n",
      "               0.0659,      0.1960],\n",
      "         [    58.8917,     -0.0006,      0.2941,  ...,      0.3319,\n",
      "               0.0599,      0.1992]]], grad_fn=<ViewBackward0>)\n",
      "tensor(2.1036, grad_fn=<MseLossBackward0>)\n",
      "tensor([[[87.0000,  0.0000,  2.5940,  ...,  0.0000,  0.0000,  0.0000]]])\n",
      "tensor([[[    51.2744,      0.0816,     -0.6698,  ...,      0.0118,\n",
      "              -0.0051,     -0.0977],\n",
      "         [    54.3943,      0.2759,     -0.0317,  ...,      0.0369,\n",
      "               0.0105,     -0.0044],\n",
      "         [    57.3119,      0.0322,      0.3530,  ...,      0.0092,\n",
      "              -0.0127,      0.0529],\n",
      "         ...,\n",
      "         [    54.0099,      0.2968,     -0.1932,  ...,     -0.0445,\n",
      "               0.0651,     -0.1091],\n",
      "         [    48.8480,      0.1972,     -0.2751,  ...,      0.0296,\n",
      "              -0.0437,     -0.0916],\n",
      "         [    28.1880,     -0.0160,     -0.0439,  ...,      0.0015,\n",
      "              -0.0135,     -0.0177]]], grad_fn=<ViewBackward0>)\n",
      "tensor(1.4229, grad_fn=<MseLossBackward0>)\n",
      "tensor([[[72.0000,  0.0000, -0.0932,  ...,  0.0000,  0.0000,  0.0000]]])\n",
      "tensor([[[    61.3720,      0.6999,      0.4596,  ...,     -0.0135,\n",
      "              -0.0838,      0.0770],\n",
      "         [    65.7774,      0.6633,      0.5953,  ...,     -0.0048,\n",
      "              -0.0791,     -0.0598],\n",
      "         [    57.4161,      0.5098,      0.5929,  ...,     -0.2139,\n",
      "              -0.0444,      0.0080],\n",
      "         ...,\n",
      "         [    53.3358,      0.6567,      0.5416,  ...,     -0.0235,\n",
      "               0.0135,      0.1667],\n",
      "         [    68.4010,      0.6896,      0.6560,  ...,     -0.0240,\n",
      "              -0.0311,     -0.0743],\n",
      "         [    52.6285,      0.6395,      0.6945,  ...,     -0.1387,\n",
      "              -0.1059,      0.2627]]], grad_fn=<ViewBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:17<00:40,  5.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.3250, grad_fn=<MseLossBackward0>)\n",
      "tensor([[[90.0000,  1.0000, -1.3975,  ...,  0.0000,  0.0000,  0.0000]]])\n",
      "tensor([[[    56.6554,      0.1947,      0.0247,  ...,      0.0298,\n",
      "              -0.0858,     -0.0812],\n",
      "         [    78.4231,      0.4030,      0.1715,  ...,      0.0720,\n",
      "              -0.0808,     -0.0739],\n",
      "         [    72.3756,      0.2020,      0.2598,  ...,      0.0620,\n",
      "              -0.1138,     -0.1021],\n",
      "         ...,\n",
      "         [    64.6823,      0.2011,      0.1277,  ...,      0.0780,\n",
      "              -0.0839,     -0.1099],\n",
      "         [    73.4735,      0.2988,      0.1893,  ...,      0.0153,\n",
      "              -0.0963,     -0.1023],\n",
      "         [    90.8132,      0.2977,      0.0933,  ...,      0.0955,\n",
      "              -0.1662,     -0.1403]]], grad_fn=<ViewBackward0>)\n",
      "tensor(4.9299, grad_fn=<MseLossBackward0>)\n",
      "tensor([[[54.0000,  0.0000,  1.1411,  ...,  0.0000,  0.0000,  0.0000]]])\n",
      "tensor([[[   59.9985,     0.4494,     0.0254,  ...,     0.1771,\n",
      "              0.1125,     0.6181],\n",
      "         [   53.3335,     0.3570,     0.0444,  ...,     0.1450,\n",
      "              0.0718,     0.5159],\n",
      "         [   71.3963,     0.5418,     0.0717,  ...,     0.1932,\n",
      "              0.1202,     0.7147],\n",
      "         ...,\n",
      "         [   67.1068,     0.4600,     0.0218,  ...,     0.1558,\n",
      "              0.0997,     0.6814],\n",
      "         [   47.5195,     0.3156,     0.0966,  ...,     0.1274,\n",
      "              0.0821,     0.4897],\n",
      "         [   38.2657,     0.2196,     0.0198,  ...,     0.1200,\n",
      "              0.0505,     0.4198]]], grad_fn=<ViewBackward0>)\n",
      "tensor(1.1476, grad_fn=<MseLossBackward0>)\n",
      "tensor([[[57.0000,  0.0000, -1.1310,  ...,  0.0000,  0.0000,  0.0000]]])\n",
      "tensor([[[    27.7249,      0.1396,      0.0265,  ...,      0.2944,\n",
      "               0.1202,      0.1215],\n",
      "         [    66.3269,     -0.1558,     -0.4214,  ...,      0.3636,\n",
      "               0.0484,      0.0756],\n",
      "         [    25.0030,     -0.0238,     -0.4312,  ...,      0.5194,\n",
      "               0.0542,     -0.1154],\n",
      "         ...,\n",
      "         [    25.5495,     -0.1728,     -0.2511,  ...,      0.2441,\n",
      "               0.0678,     -0.3772],\n",
      "         [    82.1839,     -0.0285,     -0.3130,  ...,      0.2866,\n",
      "               0.0105,     -0.0843],\n",
      "         [    29.6023,     -0.0472,     -0.0380,  ...,      0.1407,\n",
      "               0.0238,      0.0683]]], grad_fn=<ViewBackward0>)\n",
      "tensor(1.1170, grad_fn=<MseLossBackward0>)\n",
      "tensor([[[69.0000,  0.0000,  0.9425,  ...,  0.0000,  0.0000,  0.0000]]])\n",
      "tensor([[[    39.6001,      0.0923,     -0.0435,  ...,     -0.0342,\n",
      "              -0.0978,      0.1999],\n",
      "         [    63.2311,      0.1925,      0.0509,  ...,      0.0044,\n",
      "               0.0432,      0.2879],\n",
      "         [    53.4337,      0.2495,      0.1156,  ...,      0.0198,\n",
      "              -0.0308,      0.1920],\n",
      "         ...,\n",
      "         [    67.8899,      0.3044,      0.1395,  ...,      0.0302,\n",
      "              -0.0303,      0.2358],\n",
      "         [    88.5054,      0.3145,      0.1673,  ...,     -0.0312,\n",
      "               0.0274,      0.3303],\n",
      "         [    74.5129,      0.3132,      0.1151,  ...,      0.0681,\n",
      "              -0.0421,      0.3808]]], grad_fn=<ViewBackward0>)\n",
      "tensor(2.3404, grad_fn=<MseLossBackward0>)\n",
      "tensor([[[72.0000,  0.0000, -0.4507,  ...,  0.0000,  0.0000,  0.0000]]])\n",
      "tensor([[[    55.2259,      0.4703,     -0.0723,  ...,     -0.0565,\n",
      "               0.2010,      0.0657],\n",
      "         [    71.7282,      0.5323,      0.2341,  ...,     -0.0553,\n",
      "               0.2240,      0.0769],\n",
      "         [    55.3533,      0.4467,      0.0037,  ...,     -0.0012,\n",
      "               0.1614,      0.0746],\n",
      "         ...,\n",
      "         [    69.1308,      0.5424,      0.1485,  ...,     -0.0225,\n",
      "               0.1859,      0.1015],\n",
      "         [    53.0596,      0.3929,      0.1229,  ...,     -0.0264,\n",
      "               0.1388,      0.0949],\n",
      "         [    52.6648,      0.3955,      0.0724,  ...,     -0.0348,\n",
      "               0.1842,      0.0665]]], grad_fn=<ViewBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:22<00:33,  5.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6281, grad_fn=<MseLossBackward0>)\n",
      "tensor([[[62.0000,  0.0000,  1.7927,  ...,  0.0000,  1.0000,  0.0000]]])\n",
      "tensor([[[   67.0889,     0.1063,     0.5982,  ...,     0.1284,\n",
      "              0.1595,     0.1061],\n",
      "         [   35.3526,     0.1045,     0.2856,  ...,     0.0820,\n",
      "              0.1173,     0.0828],\n",
      "         [   57.9124,     0.1593,     0.4837,  ...,     0.1053,\n",
      "              0.1591,     0.1147],\n",
      "         ...,\n",
      "         [   60.8047,     0.1816,     0.6548,  ...,     0.1314,\n",
      "              0.2011,     0.0538],\n",
      "         [   61.6667,     0.1381,     0.4300,  ...,     0.1275,\n",
      "              0.1836,     0.0780],\n",
      "         [   56.8910,     0.2256,     0.5713,  ...,     0.1133,\n",
      "              0.1580,     0.0799]]], grad_fn=<ViewBackward0>)\n",
      "tensor(4.3926, grad_fn=<MseLossBackward0>)\n",
      "tensor([[[73.0000,  0.0000,  2.7281,  ...,  0.0000,  0.0000,  0.0000]]])\n",
      "tensor([[[    58.4556,      0.6258,      0.2171,  ...,      0.0994,\n",
      "              -0.0397,      0.0447],\n",
      "         [    64.0984,      0.5787,      0.0164,  ...,      0.1466,\n",
      "              -0.0437,      0.0611],\n",
      "         [    59.7121,      0.5684,      0.1403,  ...,      0.1374,\n",
      "              -0.0323,      0.0660],\n",
      "         ...,\n",
      "         [    68.6790,      0.6651,      0.0586,  ...,      0.1747,\n",
      "              -0.0651,      0.0294],\n",
      "         [    59.6852,      0.5293,      0.1711,  ...,      0.1786,\n",
      "              -0.0522,      0.0552],\n",
      "         [    57.5391,      0.5019,      0.0350,  ...,      0.1272,\n",
      "              -0.0722,      0.0537]]], grad_fn=<ViewBackward0>)\n",
      "tensor(4.6475, grad_fn=<MseLossBackward0>)\n",
      "tensor([[[76.0000,  0.0000,  3.5859,  ...,  0.0000,  0.0000,  0.0000]]])\n",
      "tensor([[[    56.9397,      0.1127,      0.2715,  ...,      0.1241,\n",
      "              -0.0098,      0.0871],\n",
      "         [    65.0927,      0.1669,      0.4133,  ...,      0.1233,\n",
      "              -0.0279,      0.1077],\n",
      "         [    47.0618,      0.1652,      0.2722,  ...,      0.0940,\n",
      "              -0.0255,      0.0637],\n",
      "         ...,\n",
      "         [    56.7565,      0.1568,      0.2826,  ...,      0.0947,\n",
      "              -0.0297,      0.0852],\n",
      "         [    41.0160,      0.1604,      0.2193,  ...,      0.0828,\n",
      "              -0.0035,      0.0851],\n",
      "         [    68.5290,      0.2299,      0.3944,  ...,      0.1406,\n",
      "              -0.0134,      0.0680]]], grad_fn=<ViewBackward0>)\n",
      "tensor(1.8123, grad_fn=<MseLossBackward0>)\n",
      "tensor([[[90.0000,  0.0000, -0.3048,  ...,  0.0000,  0.0000,  1.0000]]])\n",
      "tensor([[[    52.3185,      0.1458,      0.2142,  ...,      0.2521,\n",
      "               0.1999,      0.0460],\n",
      "         [    73.5096,      0.2013,     -0.2058,  ...,      0.4960,\n",
      "               0.2413,     -0.0487],\n",
      "         [    45.3872,      0.0857,     -0.1539,  ...,      0.2626,\n",
      "               0.1188,     -0.0469],\n",
      "         ...,\n",
      "         [    64.4983,      0.1086,      0.0253,  ...,      0.4210,\n",
      "               0.2171,     -0.0648],\n",
      "         [    61.0615,      0.2194,      0.1420,  ...,      0.3316,\n",
      "               0.1866,      0.0999],\n",
      "         [    46.3882,      0.3369,      0.1106,  ...,      0.2848,\n",
      "               0.1026,      0.0558]]], grad_fn=<ViewBackward0>)\n",
      "tensor(1.0508, grad_fn=<MseLossBackward0>)\n",
      "tensor([[[57.0000,  0.0000, -0.6944,  ...,  0.0000,  0.0000,  0.0000]]])\n",
      "tensor([[[    77.6468,      0.2133,      0.6781,  ...,      0.0674,\n",
      "               0.0661,      0.4416],\n",
      "         [    32.5999,      0.0089,      0.4124,  ...,      0.1730,\n",
      "               0.0756,      0.3754],\n",
      "         [    36.0689,      0.0984,      0.4111,  ...,      0.1386,\n",
      "               0.0943,      0.3347],\n",
      "         ...,\n",
      "         [    77.6173,      0.0982,      0.3434,  ...,      0.1657,\n",
      "               0.0879,      0.5049],\n",
      "         [    70.0071,      0.2674,      0.1561,  ...,     -0.0451,\n",
      "               0.0689,      0.3294],\n",
      "         [    69.7654,      0.3924,      0.8116,  ...,      0.1383,\n",
      "               0.0738,      0.3083]]], grad_fn=<ViewBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [00:27<00:26,  5.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.3158, grad_fn=<MseLossBackward0>)\n",
      "tensor([[[60.0000,  0.0000, -0.1458,  ...,  0.0000,  0.0000,  0.0000]]])\n",
      "tensor([[[    59.2085,     -0.1113,      0.2274,  ...,     -0.0343,\n",
      "              -0.1236,      0.0856],\n",
      "         [    55.0480,     -0.0787,      0.3507,  ...,     -0.1292,\n",
      "              -0.1091,      0.1304],\n",
      "         [    40.4069,     -0.0640,      0.4030,  ...,     -0.0095,\n",
      "              -0.0984,      0.0647],\n",
      "         ...,\n",
      "         [    72.3045,     -0.0780,      0.5292,  ...,     -0.1860,\n",
      "              -0.1316,      0.0386],\n",
      "         [    65.2534,     -0.2323,      0.2850,  ...,     -0.1103,\n",
      "              -0.1161,      0.0951],\n",
      "         [    62.6999,     -0.1073,      0.6269,  ...,     -0.1354,\n",
      "              -0.1093,      0.0544]]], grad_fn=<ViewBackward0>)\n",
      "tensor(2.3578, grad_fn=<MseLossBackward0>)\n",
      "tensor([[[68.0000,  0.0000, -0.8200,  ...,  0.0000,  0.0000,  0.0000]]])\n",
      "tensor([[[    48.5509,      0.2579,     -0.1082,  ...,      0.0543,\n",
      "               0.0569,      0.2801],\n",
      "         [    39.0456,      0.2509,      0.4062,  ...,      0.1176,\n",
      "               0.1260,      0.1405],\n",
      "         [    55.2684,      0.3100,      0.4135,  ...,      0.1181,\n",
      "               0.0549,      0.5215],\n",
      "         ...,\n",
      "         [    38.6849,      0.2077,      0.4207,  ...,      0.1854,\n",
      "               0.1919,      0.1110],\n",
      "         [    49.2644,      0.2605,     -0.0667,  ...,     -0.0101,\n",
      "               0.0322,      0.2861],\n",
      "         [    61.0493,      0.4303,      0.4138,  ...,      0.1889,\n",
      "               0.1089,      0.5460]]], grad_fn=<ViewBackward0>)\n",
      "tensor(1.2569, grad_fn=<MseLossBackward0>)\n",
      "tensor([[[63.0000,  0.0000,  0.4367,  ...,  1.0000,  0.0000,  0.0000]]])\n",
      "tensor([[[    78.2783,      0.1971,      1.1980,  ...,      0.6559,\n",
      "               0.1118,     -0.1555],\n",
      "         [    65.3019,      0.1363,      0.9175,  ...,      0.6199,\n",
      "               0.0796,     -0.1173],\n",
      "         [    57.9297,      0.2431,      0.7581,  ...,      0.4417,\n",
      "               0.0721,     -0.0569],\n",
      "         ...,\n",
      "         [    50.0607,      0.1098,      0.8338,  ...,      0.4383,\n",
      "               0.0638,     -0.0761],\n",
      "         [    61.8801,      0.1784,      0.7908,  ...,      0.5180,\n",
      "               0.0330,     -0.0764],\n",
      "         [    69.6027,      0.1344,      0.7536,  ...,      0.5393,\n",
      "               0.0876,     -0.1137]]], grad_fn=<ViewBackward0>)\n",
      "tensor(1.0054, grad_fn=<MseLossBackward0>)\n",
      "tensor([[[70.0000,  0.0000, -0.5525,  ...,  0.0000,  0.0000,  0.0000]]])\n",
      "tensor([[[    78.0380,      0.4282,     -0.1177,  ...,     -0.2013,\n",
      "              -0.0798,      0.0378],\n",
      "         [    57.4862,      0.2583,     -0.1142,  ...,     -0.1343,\n",
      "              -0.0718,      0.0489],\n",
      "         [    65.3431,      0.3708,      0.1064,  ...,     -0.1652,\n",
      "              -0.0514,      0.0649],\n",
      "         ...,\n",
      "         [    80.7855,      0.4420,     -0.0022,  ...,     -0.1584,\n",
      "              -0.0445,      0.0120],\n",
      "         [    61.5355,      0.3083,     -0.0773,  ...,     -0.1110,\n",
      "              -0.0113,      0.0041],\n",
      "         [    76.1157,      0.3848,      0.1375,  ...,     -0.1650,\n",
      "              -0.0557,      0.0732]]], grad_fn=<ViewBackward0>)\n",
      "tensor(1.0369, grad_fn=<MseLossBackward0>)\n",
      "tensor([[[79.0000,  0.0000,  1.6352,  ...,  0.0000,  0.0000,  0.0000]]])\n",
      "tensor([[[    62.1947,      0.3500,     -0.1837,  ...,     -0.0376,\n",
      "               0.0157,     -0.0416],\n",
      "         [    65.7360,      0.2653,     -0.3140,  ...,     -0.0354,\n",
      "               0.0159,     -0.1414],\n",
      "         [    35.2491,      0.2250,      0.0242,  ...,      0.0374,\n",
      "              -0.0014,     -0.0122],\n",
      "         ...,\n",
      "         [    65.5183,      0.3115,     -0.3092,  ...,     -0.0465,\n",
      "               0.0349,     -0.0675],\n",
      "         [    70.4198,      0.3946,     -0.2596,  ...,     -0.0043,\n",
      "               0.0179,     -0.0704],\n",
      "         [    53.6909,      0.2552,     -0.1260,  ...,     -0.0022,\n",
      "               0.0398,     -0.0725]]], grad_fn=<ViewBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [00:32<00:20,  5.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8511, grad_fn=<MseLossBackward0>)\n",
      "tensor([[[76.0000,  1.0000, -0.0903,  ...,  0.0000,  0.0000,  0.0000]]])\n",
      "tensor([[[    59.2018,      0.1619,      0.7460,  ...,      0.0845,\n",
      "               0.0203,      0.1572],\n",
      "         [    76.8315,      0.2324,      0.9474,  ...,      0.0114,\n",
      "               0.0456,      0.2297],\n",
      "         [    70.1700,      0.2920,      1.0035,  ...,      0.0456,\n",
      "               0.0306,      0.1538],\n",
      "         ...,\n",
      "         [    70.4930,      0.2366,      0.7661,  ...,     -0.0027,\n",
      "               0.0198,      0.1414],\n",
      "         [    77.9535,      0.3550,      1.0316,  ...,      0.0641,\n",
      "               0.0370,      0.1569],\n",
      "         [    95.0761,      0.2743,      1.0437,  ...,      0.0978,\n",
      "               0.0395,      0.1399]]], grad_fn=<ViewBackward0>)\n",
      "tensor(2.2967, grad_fn=<MseLossBackward0>)\n",
      "tensor([[[75.0000,  0.0000,  0.4331,  ...,  0.0000,  1.0000,  0.0000]]])\n",
      "tensor([[[    56.1272,      0.4327,      0.7006,  ...,      0.0304,\n",
      "               0.0265,      0.0804],\n",
      "         [    38.6056,      0.2951,      0.4612,  ...,      0.0368,\n",
      "               0.0275,      0.1115],\n",
      "         [    41.8810,      0.3585,      0.6382,  ...,      0.0492,\n",
      "               0.0392,      0.1080],\n",
      "         ...,\n",
      "         [    56.6115,      0.4394,      0.6291,  ...,      0.0187,\n",
      "               0.0233,      0.0592],\n",
      "         [    83.0457,      0.5986,      0.9236,  ...,      0.0125,\n",
      "               0.0148,      0.0740],\n",
      "         [    61.5639,      0.5030,      0.7207,  ...,      0.0208,\n",
      "              -0.0017,      0.0971]]], grad_fn=<ViewBackward0>)\n",
      "tensor(2.9530, grad_fn=<MseLossBackward0>)\n",
      "tensor([[[72.0000,  0.0000, -0.4507,  ...,  0.0000,  0.0000,  0.0000]]])\n",
      "tensor([[[    43.3406,     -0.2796,     -0.0620,  ...,      0.1141,\n",
      "              -0.3072,     -0.0066],\n",
      "         [    56.1218,     -0.3355,     -0.6459,  ...,      0.4253,\n",
      "              -0.1520,     -0.0923],\n",
      "         [    60.1354,     -0.2017,     -0.2778,  ...,      0.1331,\n",
      "              -0.2923,      0.0118],\n",
      "         ...,\n",
      "         [   143.7496,     -0.3505,     -0.9621,  ...,      0.1815,\n",
      "              -0.4074,      0.1226],\n",
      "         [    73.4358,      0.0673,     -0.8284,  ...,      0.2077,\n",
      "               0.0309,      0.0566],\n",
      "         [    74.0700,     -0.4500,     -0.6055,  ...,      0.3709,\n",
      "              -0.2686,     -0.0588]]], grad_fn=<ViewBackward0>)\n",
      "tensor(1.7581, grad_fn=<MseLossBackward0>)\n",
      "tensor([[[81.0000,  0.0000,  1.4191,  ...,  0.0000,  0.0000,  0.0000]]])\n",
      "tensor([[[    49.9084,      0.1605,     -0.0677,  ...,     -0.0232,\n",
      "              -0.1254,      0.2812],\n",
      "         [    45.4251,      0.1182,     -0.2190,  ...,     -0.1633,\n",
      "              -0.1421,      0.3093],\n",
      "         [    57.3694,      0.2008,     -0.2823,  ...,     -0.0634,\n",
      "              -0.1055,      0.2336],\n",
      "         ...,\n",
      "         [    55.9078,      0.1704,     -0.0863,  ...,      0.0014,\n",
      "              -0.1302,      0.3569],\n",
      "         [    51.2089,      0.1675,     -0.1271,  ...,     -0.0475,\n",
      "              -0.1383,      0.2952],\n",
      "         [    47.7510,      0.1388,     -0.1693,  ...,     -0.0388,\n",
      "              -0.1195,      0.3135]]], grad_fn=<ViewBackward0>)\n",
      "tensor(0.8361, grad_fn=<MseLossBackward0>)\n",
      "tensor([[[70.0000,  0.0000, -0.3511,  ...,  0.0000,  0.0000,  0.0000]]])\n",
      "tensor([[[    71.4901,      0.5240,      0.6509,  ...,      0.2219,\n",
      "               0.0883,      0.1503],\n",
      "         [    58.3186,      0.3195,      0.5355,  ...,      0.0709,\n",
      "              -0.0548,     -0.0432],\n",
      "         [    48.1810,      0.2735,      0.5768,  ...,     -0.0082,\n",
      "               0.0003,      0.0517],\n",
      "         ...,\n",
      "         [    64.2888,      0.4843,      0.5276,  ...,      0.1106,\n",
      "               0.0090,      0.0209],\n",
      "         [    57.3646,      0.3914,      0.4226,  ...,      0.1299,\n",
      "               0.0208,      0.1265],\n",
      "         [    51.4172,      0.2892,      0.4932,  ...,      0.0808,\n",
      "              -0.0097,      0.1111]]], grad_fn=<ViewBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [00:37<00:15,  5.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7166, grad_fn=<MseLossBackward0>)\n",
      "tensor([[[50.0000,  0.0000,  0.3542,  ...,  0.0000,  0.0000,  0.0000]]])\n",
      "tensor([[[    77.4723,      0.6541,     -0.0931,  ...,      0.2203,\n",
      "               0.1727,      0.5865],\n",
      "         [    67.8820,      0.4855,     -0.0405,  ...,      0.1503,\n",
      "               0.1269,      0.4843],\n",
      "         [    42.5054,      0.4351,      0.0122,  ...,      0.1304,\n",
      "               0.1186,      0.3043],\n",
      "         ...,\n",
      "         [    52.6750,      0.3535,      0.0091,  ...,      0.1441,\n",
      "               0.0694,      0.3689],\n",
      "         [    65.5573,      0.5618,      0.0367,  ...,      0.2242,\n",
      "               0.1227,      0.3993],\n",
      "         [    65.2112,      0.5782,     -0.1814,  ...,      0.1645,\n",
      "               0.1913,      0.4989]]], grad_fn=<ViewBackward0>)\n",
      "tensor(2.3678, grad_fn=<MseLossBackward0>)\n",
      "tensor([[[57.0000,  0.0000, -0.2452,  ...,  1.0000,  0.0000,  0.0000]]])\n",
      "tensor([[[   113.4000,      1.2569,      1.2394,  ...,      0.4033,\n",
      "               0.0184,      0.8440],\n",
      "         [    85.3419,      0.2809,      0.5150,  ...,      0.3839,\n",
      "               0.0062,      0.1721],\n",
      "         [    81.1661,      1.0514,      1.1142,  ...,      0.3613,\n",
      "              -0.2416,      0.6139],\n",
      "         ...,\n",
      "         [    84.6117,      1.2400,      0.8981,  ...,      0.2993,\n",
      "              -0.0374,      0.5168],\n",
      "         [    67.9602,      0.2445,      0.2822,  ...,      0.3068,\n",
      "               0.0316,      0.2455],\n",
      "         [    50.5063,      0.2332,      0.5090,  ...,      0.1482,\n",
      "               0.1127,      0.1208]]], grad_fn=<ViewBackward0>)\n",
      "tensor(1.8365, grad_fn=<MseLossBackward0>)\n",
      "tensor([[[65.0000,  0.0000,  0.4114,  ...,  0.0000,  0.0000,  0.0000]]])\n",
      "tensor([[[    66.5982,      0.0027,      0.0694,  ...,     -0.1274,\n",
      "              -0.0911,      0.1284],\n",
      "         [    53.0827,     -0.1976,     -0.0628,  ...,     -0.1373,\n",
      "              -0.1171,      0.0897],\n",
      "         [    54.7675,     -0.1373,      0.1032,  ...,     -0.1446,\n",
      "              -0.1060,      0.1083],\n",
      "         ...,\n",
      "         [    44.2467,      0.0803,      0.1609,  ...,     -0.0488,\n",
      "              -0.0366,      0.1250],\n",
      "         [    53.5291,     -0.1422,      0.2459,  ...,     -0.0827,\n",
      "              -0.0937,      0.1445],\n",
      "         [    57.1206,     -0.0307,      0.1047,  ...,     -0.1496,\n",
      "              -0.0192,      0.1249]]], grad_fn=<ViewBackward0>)\n",
      "tensor(2.6283, grad_fn=<MseLossBackward0>)\n",
      "tensor([[[84.0000,  0.0000, -1.5030,  ...,  0.0000,  0.0000,  0.0000]]])\n",
      "tensor([[[    64.9184,      0.3768,      0.1556,  ...,      0.2821,\n",
      "               0.2851,      0.0254],\n",
      "         [    76.9107,      0.4456,      0.1382,  ...,      0.3966,\n",
      "               0.3396,     -0.0025],\n",
      "         [    69.5897,      0.2801,      0.1522,  ...,      0.2512,\n",
      "               0.1612,      0.0361],\n",
      "         ...,\n",
      "         [    55.8241,      0.2567,      0.1204,  ...,      0.2262,\n",
      "               0.1807,      0.0507],\n",
      "         [    69.5780,      0.0848,      0.0838,  ...,      0.2795,\n",
      "               0.1388,     -0.0457],\n",
      "         [    63.8027,      0.2686,      0.0105,  ...,      0.2331,\n",
      "               0.2678,      0.0008]]], grad_fn=<ViewBackward0>)\n",
      "tensor(1.2227, grad_fn=<MseLossBackward0>)\n",
      "tensor([[[67.0000,  1.0000,  0.1768,  ...,  0.0000,  0.0000,  0.0000]]])\n",
      "tensor([[[    65.6430,      0.4688,      0.6194,  ...,     -0.0327,\n",
      "               0.0954,     -0.0334],\n",
      "         [    46.3125,      0.3137,      0.4274,  ...,     -0.0065,\n",
      "               0.0478,     -0.0125],\n",
      "         [    60.4546,      0.3081,      0.6897,  ...,     -0.0567,\n",
      "              -0.0108,     -0.0059],\n",
      "         ...,\n",
      "         [    54.5329,      0.2797,      0.6021,  ...,     -0.0333,\n",
      "              -0.0036,      0.0259],\n",
      "         [    50.4817,      0.2732,      0.5942,  ...,     -0.0064,\n",
      "               0.0733,      0.0346],\n",
      "         [    54.5388,      0.2848,      0.4070,  ...,     -0.0626,\n",
      "              -0.0162,     -0.0201]]], grad_fn=<ViewBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [00:43<00:10,  5.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0333, grad_fn=<MseLossBackward0>)\n",
      "tensor([[[65.0000,  0.0000,  0.2180,  ...,  0.0000,  0.0000,  0.0000]]])\n",
      "tensor([[[    76.0294,      0.3167,      1.4031,  ...,      0.0335,\n",
      "               0.2552,     -0.0145],\n",
      "         [    46.4658,      0.2878,      0.8713,  ...,      0.0194,\n",
      "               0.1253,      0.0909],\n",
      "         [    53.4063,      0.2968,      0.9068,  ...,     -0.0083,\n",
      "               0.1882,      0.0103],\n",
      "         ...,\n",
      "         [    54.3103,      0.2933,      0.8695,  ...,      0.0692,\n",
      "               0.1406,      0.0473],\n",
      "         [    58.9399,      0.1696,      0.9165,  ...,     -0.0187,\n",
      "               0.1197,      0.0007],\n",
      "         [    48.9617,      0.2630,      0.9389,  ...,     -0.0358,\n",
      "               0.0384,      0.0333]]], grad_fn=<ViewBackward0>)\n",
      "tensor(1.3296, grad_fn=<MseLossBackward0>)\n",
      "tensor([[[66.0000,  0.0000,  2.0546,  ...,  0.0000,  0.0000,  0.0000]]])\n",
      "tensor([[[   76.3845,     0.3484,     0.8452,  ...,     0.0563,\n",
      "              0.5274,     0.1549],\n",
      "         [   50.7710,     0.1479,     0.5685,  ...,     0.0704,\n",
      "              0.3731,     0.1277],\n",
      "         [   55.4793,     0.3505,     0.5786,  ...,     0.0798,\n",
      "              0.3693,     0.1397],\n",
      "         ...,\n",
      "         [   58.3017,     0.3177,     0.7446,  ...,     0.0889,\n",
      "              0.4361,     0.2073],\n",
      "         [   80.0771,     0.3327,     0.8022,  ...,     0.0786,\n",
      "              0.5783,     0.1495],\n",
      "         [   64.0462,     0.2365,     0.6388,  ...,     0.0236,\n",
      "              0.3833,     0.0872]]], grad_fn=<ViewBackward0>)\n",
      "tensor(0.8793, grad_fn=<MseLossBackward0>)\n",
      "tensor([[[76.0000,  1.0000, -0.0903,  ...,  0.0000,  0.0000,  0.0000]]])\n",
      "tensor([[[    68.9275,     -0.0213,      0.6915,  ...,      0.0421,\n",
      "               0.2260,      0.1569],\n",
      "         [    44.0270,      0.1794,      0.6569,  ...,      0.0185,\n",
      "               0.1746,      0.1203],\n",
      "         [    55.4079,      0.0283,      0.6258,  ...,      0.0592,\n",
      "               0.1195,      0.1009],\n",
      "         ...,\n",
      "         [    61.3919,      0.1414,      0.8466,  ...,      0.0948,\n",
      "               0.2104,      0.1028],\n",
      "         [    63.4558,     -0.0049,      0.6215,  ...,     -0.0214,\n",
      "               0.1720,      0.0592],\n",
      "         [    50.4001,      0.0971,      0.6253,  ...,      0.0496,\n",
      "               0.1307,      0.1268]]], grad_fn=<ViewBackward0>)\n",
      "tensor(0.6206, grad_fn=<MseLossBackward0>)\n",
      "tensor([[[62.0000,  0.0000,  1.7927,  ...,  0.0000,  1.0000,  0.0000]]])\n",
      "tensor([[[    63.6975,      0.3061,      0.2049,  ...,     -0.0679,\n",
      "               0.1425,      0.0980],\n",
      "         [    73.7954,      0.4094,      0.5189,  ...,     -0.0492,\n",
      "               0.1446,      0.0995],\n",
      "         [    53.8781,      0.5092,      0.5994,  ...,      0.0112,\n",
      "               0.1785,      0.0945],\n",
      "         ...,\n",
      "         [    56.7830,      0.2926,      0.1962,  ...,     -0.0587,\n",
      "               0.1135,      0.0789],\n",
      "         [    66.8932,      0.3822,      0.0353,  ...,     -0.0735,\n",
      "               0.1712,      0.0678],\n",
      "         [    69.1067,      0.3341,      0.2311,  ...,     -0.1443,\n",
      "               0.1025,      0.1076]]], grad_fn=<ViewBackward0>)\n",
      "tensor(2.1593, grad_fn=<MseLossBackward0>)\n",
      "tensor([[[57.0000,  0.0000, -0.2452,  ...,  1.0000,  0.0000,  0.0000]]])\n",
      "tensor([[[    47.7072,      0.2637,      0.0630,  ...,     -0.2569,\n",
      "              -0.0116,     -0.0057],\n",
      "         [    47.9415,      0.6489,      0.1936,  ...,      0.1055,\n",
      "              -0.1620,      0.1189],\n",
      "         [    66.3932,      1.2156,      0.2203,  ...,      0.2884,\n",
      "              -0.0664,      0.2510],\n",
      "         ...,\n",
      "         [    60.4542,      0.8423,      0.1839,  ...,      0.0681,\n",
      "              -0.1397,      0.1130],\n",
      "         [    57.9494,      0.1021,     -0.2152,  ...,     -0.3246,\n",
      "              -0.0905,     -0.2429],\n",
      "         [    78.8003,      0.3496,      0.0335,  ...,     -0.1010,\n",
      "              -0.0751,     -0.0026]]], grad_fn=<ViewBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [00:48<00:05,  5.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1931, grad_fn=<MseLossBackward0>)\n",
      "tensor([[[69.0000,  1.0000,  1.0457,  ...,  0.0000,  0.0000,  0.0000]]])\n",
      "tensor([[[    57.4249,      0.6660,      0.1118,  ...,      0.3605,\n",
      "               0.1569,      0.2772],\n",
      "         [    68.2448,      0.4291,      0.1188,  ...,      0.1224,\n",
      "               0.1939,      0.0949],\n",
      "         [    57.1778,      0.2097,      0.2779,  ...,      0.0536,\n",
      "               0.0017,      0.1204],\n",
      "         ...,\n",
      "         [    53.6904,      0.1413,      0.1187,  ...,      0.0085,\n",
      "               0.1349,     -0.0009],\n",
      "         [    76.3008,      0.6065,      0.1208,  ...,      0.3842,\n",
      "               0.1497,      0.1964],\n",
      "         [    41.7606,      0.1448,      0.0315,  ...,      0.0707,\n",
      "               0.1008,      0.0910]]], grad_fn=<ViewBackward0>)\n",
      "tensor(1.9294, grad_fn=<MseLossBackward0>)\n",
      "tensor([[[74.0000,  0.0000, -0.0806,  ...,  0.0000,  0.0000,  0.0000]]])\n",
      "tensor([[[    46.0793,      0.1049,      0.2510,  ...,      0.2301,\n",
      "              -0.1970,     -0.0287],\n",
      "         [    67.1383,     -0.0404,      0.1715,  ...,      0.3685,\n",
      "              -0.1478,     -0.1825],\n",
      "         [    67.3426,      0.0231,      0.4117,  ...,      0.3813,\n",
      "              -0.0455,      0.0435],\n",
      "         ...,\n",
      "         [    71.4545,      0.1656,      0.0902,  ...,      0.5271,\n",
      "              -0.2013,     -0.1800],\n",
      "         [    88.0646,      0.0732,      0.1528,  ...,      0.6991,\n",
      "              -0.2115,     -0.1766],\n",
      "         [    61.7637,      0.1708,      0.0629,  ...,      0.3436,\n",
      "              -0.1001,     -0.0483]]], grad_fn=<ViewBackward0>)\n",
      "tensor(623.9772, grad_fn=<MseLossBackward0>)\n",
      "tensor([[[76.0000,  1.0000,  0.4803,  ...,  0.0000,  0.0000,  0.0000]]])\n",
      "tensor([[[    73.0414,      0.4975,      0.3963,  ...,     -0.3221,\n",
      "               0.1410,      0.3159],\n",
      "         [    69.9988,      0.5581,      0.0683,  ...,      0.7137,\n",
      "              -0.1579,      0.2029],\n",
      "         [    41.1039,      0.1854,      0.2658,  ...,      0.2336,\n",
      "              -0.0997,      0.0736],\n",
      "         ...,\n",
      "         [    76.1482,      0.4472,      0.1131,  ...,     -0.2636,\n",
      "               0.1004,      0.2963],\n",
      "         [    67.9338,      0.3318,      0.4615,  ...,     -0.4390,\n",
      "               0.1377,      0.1830],\n",
      "         [    54.7170,      0.5080,      0.2280,  ...,      0.3633,\n",
      "              -0.1010,      0.1675]]], grad_fn=<ViewBackward0>)\n",
      "tensor(1.3345, grad_fn=<MseLossBackward0>)\n",
      "tensor([[[75.0000,  0.0000, -0.5321,  ...,  0.0000,  0.0000,  0.0000]]])\n",
      "tensor([[[    54.9919,      0.4024,      0.2414,  ...,      0.0530,\n",
      "               0.1706,      0.1657],\n",
      "         [    62.4490,      0.4519,      0.0597,  ...,      0.0366,\n",
      "               0.1361,      0.1242],\n",
      "         [    59.8994,      0.3887,      0.1890,  ...,     -0.0130,\n",
      "               0.1827,      0.0919],\n",
      "         ...,\n",
      "         [    87.4089,      0.5066,      0.0341,  ...,     -0.0311,\n",
      "               0.0800,      0.1604],\n",
      "         [    44.8050,      0.4076,      0.3162,  ...,      0.0635,\n",
      "               0.1617,      0.1227],\n",
      "         [    58.2620,      0.3401,      0.0505,  ...,     -0.0224,\n",
      "               0.1316,      0.1127]]], grad_fn=<ViewBackward0>)\n",
      "tensor(1.4074, grad_fn=<MseLossBackward0>)\n",
      "tensor([[[87.0000,  0.0000,  1.0530,  ...,  0.0000,  0.0000,  0.0000]]])\n",
      "tensor([[[    47.5764,      0.1790,      0.1260,  ...,      0.1323,\n",
      "               0.1839,     -0.1106],\n",
      "         [    55.4493,      0.1717,      0.2204,  ...,      0.1496,\n",
      "               0.0899,     -0.0920],\n",
      "         [    55.9850,      0.2054,     -0.1498,  ...,      0.1821,\n",
      "               0.1165,     -0.1388],\n",
      "         ...,\n",
      "         [    53.1173,      0.1504,     -0.0251,  ...,      0.0859,\n",
      "               0.1366,     -0.1526],\n",
      "         [    55.1913,      0.2892,      0.0722,  ...,      0.1605,\n",
      "               0.1234,     -0.0772],\n",
      "         [    52.1005,      0.4919,     -0.0332,  ...,      0.2662,\n",
      "               0.2306,     -0.1529]]], grad_fn=<ViewBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:53<00:00,  5.39s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "\n",
    "\n",
    "def pretrain_loop(\n",
    "        data: TCGADataset,\n",
    "        batch_size: int, \n",
    "    ):\n",
    "    \n",
    "    loader = DataLoader(\n",
    "        data, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True, \n",
    "        num_workers=multiprocessing.cpu_count()-1\n",
    "    )\n",
    "    [omic_sample], _, _, _ = next(iter(loader))   \n",
    "    \n",
    "    \n",
    "    model = TabPretrainer(\n",
    "        sample = omic_sample,\n",
    "        # input_channels=omic_sample.shape[-1], \n",
    "        input_axis=1, \n",
    "        latent_shape=[256, 32], \n",
    "        attn_dropout=0.1, \n",
    "        num_heads=4,\n",
    "        num_freq_bands=8\n",
    "    )\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    \n",
    "    loss_fn = TabularLoss(method=\"mse\")\n",
    "    \n",
    "    for epoch in tqdm(range(10)):\n",
    "        for idx, batch in enumerate(loader):\n",
    "            [omic], censorship, event_time, y_disc = batch\n",
    "            rec_omic = model(omic)\n",
    "            loss = loss_fn(omic, rec_omic)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # print every 10th batch\n",
    "            if idx % 100 == 0:\n",
    "                print(loss)\n",
    "                print(omic)\n",
    "                print(rec_omic)\n",
    "            \n",
    "    \n",
    "pretrain_loop(\n",
    "    data=blca, \n",
    "    batch_size=1)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-17T13:15:54.253071Z",
     "start_time": "2023-10-17T13:14:59.899710Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-17T09:47:54.724543Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-17T09:47:54.725007Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-17T09:47:54.725311Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cognition",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
