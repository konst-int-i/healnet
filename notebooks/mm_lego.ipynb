{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-01T14:16:39.017279Z",
     "start_time": "2023-11-01T14:16:38.846660Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>.container { width:100% !important; }</style>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "if \"x_perceiver\" not in os.listdir():\n",
    "    os.chdir(\"/home/kh701/pycharm/healnet/\")\n",
    "import torch\n",
    "from torch import nn\n",
    "import multiprocessing\n",
    "from typing import *\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "import einops\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from healnet.models.explainer import Explainer\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "\n",
    "from healnet.utils import Config, flatten_config\n",
    "from healnet.etl import TCGADataset\n",
    "from healnet.models.healnet import Attention, PreNorm\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "    \n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-01T14:16:51.216308Z",
     "start_time": "2023-11-01T14:16:44.330188Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filled 0 missing values with mean\n",
      "Missing values per feature: \n",
      " Series([], dtype: int64)\n",
      "Slides available: 436\n",
      "Omic available: 437\n",
      "Overlap: 436\n",
      "Filtering out 1 samples for which there are no omic data available\n",
      "Dataloader initialised for blca dataset\n",
      "Dataset: BLCA\n",
      "Molecular data shape: (436, 2191)\n",
      "Molecular/Slide match: 436/436\n",
      "Slide level count: 4\n",
      "Slide level dimensions: ((79968, 79653), (19992, 19913), (4998, 4978), (2499, 2489))\n",
      "Slide resize dimensions: w: 1024, h: 1024\n",
      "Sources selected: ['omic']\n",
      "Censored share: 0.539\n",
      "Survival_bin_sizes: {0: 72, 1: 83, 2: 109, 3: 172}\n",
      "Filled 0 missing values with mean\n",
      "Missing values per feature: \n",
      " Series([], dtype: int64)\n",
      "Slides available: 1019\n",
      "Omic available: 1022\n",
      "Overlap: 1019\n",
      "Filtering out 3 samples for which there are no omic data available\n",
      "Dataloader initialised for brca dataset\n",
      "Dataset: BRCA\n",
      "Molecular data shape: (1019, 2922)\n",
      "Molecular/Slide match: 1019/1019\n",
      "Slide level count: 3\n",
      "Slide level dimensions: ((35855, 34985), (8963, 8746), (2240, 2186))\n",
      "Slide resize dimensions: w: 1024, h: 1024\n",
      "Sources selected: ['omic']\n",
      "Censored share: 0.868\n",
      "Survival_bin_sizes: {3: 155, 2: 172, 1: 289, 0: 403}\n"
     ]
    }
   ],
   "source": [
    "# get dataloaders\n",
    "config = Config(\"config/main_gpu.yml\").read()\n",
    "config = flatten_config(config) # TODO - refactor to other \n",
    "\n",
    "blca = TCGADataset(\n",
    "    dataset=\"blca\", \n",
    "    config=config, \n",
    "    level=2, \n",
    "    sources=[\"omic\"]\n",
    ")\n",
    "\n",
    "brca = TCGADataset(\n",
    "    dataset=\"brca\", \n",
    "    config=config, \n",
    "    level=2, \n",
    "    sources=[\"omic\"]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "# get tabular data\n",
    "blca_loader = DataLoader(\n",
    "    blca, \n",
    "    batch_size=1, \n",
    "    shuffle=True, \n",
    "    num_workers=multiprocessing.cpu_count()-1\n",
    ")\n",
    "[sample], censorship, event_time, y_disc = next(iter(blca_loader))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-01T14:16:51.803452Z",
     "start_time": "2023-11-01T14:16:51.140604Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 1, 2183])"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-01T14:16:51.883388Z",
     "start_time": "2023-11-01T14:16:51.800281Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tabular self-supervised pre-training\n",
    "\n",
    "To start with, we want to build and encoder-decoder model which trains a cross-attention unit as the encoder, which can later on be deployed in the iterative model. We then want to benchmark the performance with pan-cancer pre-training vs. without pre-training. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "def get_cat_idx(t: torch.Tensor, n_unique: int=20) -> List[int]: \n",
    "    \"\"\"\n",
    "    Function that takes in a tensor and returns the indices of categorical variables (tensor dimension) \n",
    "    A tensor is considered categorical if it has less than n_unique unique values.\n",
    "    Args:\n",
    "        t (torch.Tensor): \n",
    "        n_unique (int): threshold for unique values\n",
    "\n",
    "    Returns:\n",
    "        Tuple(List[int], List[int]): indices of categorical and continuous variables\n",
    "    \"\"\"\n",
    "    cat_idx: List[int] = []\n",
    "    cont_idx: List[int] = []\n",
    "    for idx, col in enumerate(t.T): \n",
    "        if len(torch.unique(col)) < n_unique:\n",
    "            cat_idx.append(idx)\n",
    "        else: \n",
    "            cont_idx.append(idx)\n",
    "    return cat_idx, cont_idx"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-01T14:16:51.979756Z",
     "start_time": "2023-11-01T14:16:51.878244Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "class AttentionEncoder(nn.Module): \n",
    "    \"\"\"\n",
    "    Simple encoder that uses fourier encoding, pre-norm and cross-attention to encode the input features into a latent array \n",
    "    of size (num_latents x latent_dim). Takes in both the input tensors as well as a randomly initialised latent \n",
    "    array as the input. \n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 x_c: int,  # input channels\n",
    "                 x_d: int,  # channel_dims\n",
    "                 latent: torch.Tensor,\n",
    "                 input_axis: int = 1,\n",
    "                 attn_dropout: float = 0.1,\n",
    "                 num_heads: int = 4,\n",
    "                 num_freq_bands: int=8,\n",
    "                 ):    \n",
    "        super().__init__()\n",
    "        \n",
    "        self.channel_dims = x_d\n",
    "        self.input_axis = input_axis\n",
    "        self.attn_dropout = attn_dropout\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        \n",
    "        # fourier_channels = (input_axis * ((num_freq_bands * 2) + 1))\n",
    "        # input_dim = fourier_channels + input_channels\n",
    "        input_dim = x_d * x_c\n",
    "        \n",
    "        l_c, l_d = latent.shape\n",
    "        print(l_d, input_dim)\n",
    "        enc = PreNorm(l_d, Attention(l_d, input_dim, heads=num_heads, dim_head=num_heads, dropout=attn_dropout), context_dim=l_d)\n",
    "        # enc = Attention(query_dim=input_dim, num_latents=num_latents, latent_dim=latent_dim, heads=num_heads, dim_head=num_heads, dropout=attn_dropout)\n",
    "        print(f\"Input channels {x_c}, latent dim {l_d}\")\n",
    "        \n",
    "        # attn = LatentCrossAttention(query_dim=input_channels, latent_dim=latent_dim)\n",
    "        # norm = nn.InstanceNorm1d(latent_dim)\n",
    "        # enc = PreNorm\n",
    "        # enc = PreNorm(latent_dim, Attention(query_dim=input_dim, context_dim=latent_dim, heads=num_heads, dim_head=num_heads, dropout=attn_dropout), context_dim=latent_dim)\n",
    "        self.layers = nn.ModuleList([enc])\n",
    "        print(self.layers)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, latent: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Note: context is the data, x is the latent\n",
    "        Args:\n",
    "            latent: \n",
    "            context: \n",
    "\n",
    "        Returns:\n",
    "\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            # note old setup - latent is still the query, context is provided by modality\n",
    "            latent = layer(query=latent, context=x)\n",
    "            \n",
    "        return latent\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-01T14:18:13.783136Z",
     "start_time": "2023-11-01T14:18:13.671891Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The decoder often needs to be different depending on the modality, so let's implement modality-specific decoders while trying to have a relatively general-purpose encoder that we can plug into the pipeline.\n",
    "\n",
    "Note that we may change this later down the line. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-01T14:16:52.154343Z",
     "start_time": "2023-11-01T14:16:52.074107Z"
    }
   },
   "outputs": [],
   "source": [
    "class TabularDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder suited for tabular data. We use the following: \n",
    "    - Skip connections: faster and more stable training\n",
    "    - Batch normalisation: stabilises the activations and speeds up training\n",
    "    - Activation: Output layer to map back to output dimensions, corresponding to the original data dims\n",
    "    Tries to reconstruct the original input given the latent\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_dim: int, num_latents: int, output_dim: int, method: str = \"dense\"):\n",
    "        super(TabularDecoder, self).__init__()\n",
    "        assert method in [\"dense\", \"conv\"], \"Decoder type not recognised\"\n",
    "        # check that latent_dim is divisible by 4\n",
    "        assert num_latents % 4 == 0, \"Latent dim must be a multiple of 4\"\n",
    "        layers = []\n",
    "        \n",
    "        if method == \"dense\": \n",
    "            \n",
    "            # flatten latent array (batch, num_latents, latent_dim) -> (batch, num_latents * latent_dim)\n",
    "            layers.extend([nn.Flatten()]) \n",
    "            out_dims = [1024, 512, 256] # may refactor as hyperparameter later\n",
    "            \n",
    "            in_dim = latent_dim * num_latents\n",
    "            for idx, out_dim in enumerate(out_dims):\n",
    "                \n",
    "                layers.extend([\n",
    "                    nn.Linear(in_features=in_dim, out_features=out_dim), \n",
    "                    nn.LeakyReLU(), \n",
    "                    nn.InstanceNorm1d(out_dim, track_running_stats=False), \n",
    "                    nn.Dropout(0.5)\n",
    "                ])\n",
    "                \n",
    "                in_dim = out_dim # update for next layer\n",
    "            \n",
    "            # final layer to reconstruct output\n",
    "            layers.append(nn.Linear(in_dim, output_dim))\n",
    "        \n",
    "        elif method == \"conv\": \n",
    "            print(latent_dim, num_latents)\n",
    "            layers.extend([\n",
    "                nn.ConvTranspose1d(num_latents, out_channels=int(num_latents/2), kernel_size=4, stride=2, padding=1), \n",
    "                nn.BatchNorm1d(int(num_latents/2)),\n",
    "                nn.LeakyReLU(negative_slope=0.1),\n",
    "                \n",
    "                nn.ConvTranspose1d(int(num_latents/2), out_channels=int(num_latents/4), kernel_size=4, stride=2, padding=1),\n",
    "                nn.BatchNorm1d(int(num_latents/4)),\n",
    "                nn.LeakyReLU(negative_slope=0.1),\n",
    "                \n",
    "                # If you added any other ConvTranspose layers, ensure the channel sizes match correctly for those as well.\n",
    "                \n",
    "                nn.Conv1d(int(num_latents/4), out_channels=1, kernel_size=1, stride=1, padding=0)\n",
    "            ])\n",
    "        \n",
    "        self.decode = nn.Sequential(*layers)\n",
    "        print(self.decode)\n",
    "        \n",
    "    def forward(self, latent: torch.Tensor):\n",
    "        return self.decode(latent)\n",
    "    \n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, putting it all together in the encoder-decoder model\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "from typing import *\n",
    "\n",
    "class TabPretrainer(nn.Module): \n",
    "    \"\"\"\n",
    "    Encoder-decoder model for pre-training tabular data.\n",
    "    # TODO - refactor abstract base class for initialisations \n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 sample: torch.Tensor,\n",
    "                 latent_shape: List[int],\n",
    "                 input_axis: int = 1,\n",
    "                 attn_dropout: float = 0.1,\n",
    "                 num_heads: int = 4,\n",
    "                 num_freq_bands: int=8,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.x_c = sample.shape[-2] # number of channels\n",
    "        self.x_d = sample.shape[-1] # dims per channel (features for tabular data) \n",
    "        self.input_axis = input_axis\n",
    "        self.l_c, self.l_d = latent_shape  # (c x d) [256, 32]\n",
    "        self.attn_dropout = attn_dropout\n",
    "        self.num_heads = num_heads\n",
    "        self.num_freq_bands = num_freq_bands\n",
    "        \n",
    "        \n",
    "        # randomly initialise latent\n",
    "        self.latent = nn.Parameter(torch.randn(self.l_c, self.l_d))\n",
    "        \n",
    "        # encoder\n",
    "        self.encoder = AttentionEncoder(\n",
    "            x_c=self.x_c, \n",
    "            x_d=self.x_d,\n",
    "            latent=self.latent,\n",
    "            input_axis=self.input_axis, \n",
    "            attn_dropout=attn_dropout, \n",
    "            num_heads=num_heads, \n",
    "            num_freq_bands=num_freq_bands\n",
    "        )\n",
    "        \n",
    "        # decoder\n",
    "        self.decoder = TabularDecoder(\n",
    "            latent_dim=self.l_d,\n",
    "            num_latents=self.l_c,\n",
    "            output_dim=self.x_c,\n",
    "            method=\"dense\" # using simple encoder to force good representation\n",
    "        )\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \n",
    "        # expand latent to batch size\n",
    "        if len(self.latent.shape) == 2:\n",
    "            b = x.shape[0]\n",
    "            self.latent = nn.Parameter(einops.repeat(self.latent, \"n d -> b n d\", b=b))\n",
    "        \n",
    "        # encode\n",
    "        # works much better with skip connections\n",
    "        self.latent.data = self.encoder(x=x, latent=self.latent).data\n",
    "        rec_x = self.decoder(self.latent)\n",
    "        return rec_x\n",
    "    \n",
    "    def get_latent(self):\n",
    "        return self.latent"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-01T14:16:52.257541Z",
     "start_time": "2023-11-01T14:16:52.143804Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we need to think about tabular loss functions. Here, we can explore both reconstruction losses and contrastive losses. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "class TabularLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Reconstruction loss functions for tabular data. We use two types which are commonly used with continuous data: \n",
    "    - Mean squared error\n",
    "    - Constrastive loss, measured as cosine distance between the original and reconstructed data\n",
    "    We seek to minimise both objectives.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 method: str = \"mse\",\n",
    "                 reduction: str = \"mean\",\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        assert method in [\"mse\", \"contrastive\"], \"Loss type not recognised\"\n",
    "        self.loss_type = method\n",
    "        self.reduction = reduction\n",
    "        \n",
    "        if method == \"mse\":\n",
    "            self.loss = nn.MSELoss(reduction=reduction)\n",
    "        elif method == \"contrastive\":\n",
    "            self.loss = nn.CosineEmbeddingLoss(reduction=reduction)\n",
    "            \n",
    "    def __call__(self, **kwargs):\n",
    "        return self.loss(**kwargs)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-01T14:16:52.327500Z",
     "start_time": "2023-11-01T14:16:52.230389Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, we write a pre-training loop that we can use for pre-training across cancer sites. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 2183\n",
      "Input channels 1, latent dim 32\n",
      "ModuleList(\n",
      "  (0): PreNorm(\n",
      "    (fn): Attention(\n",
      "      (to_q): Linear(in_features=32, out_features=64, bias=False)\n",
      "      (to_kv): Linear(in_features=2183, out_features=128, bias=False)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (to_out): Sequential(\n",
      "        (0): Linear(in_features=64, out_features=32, bias=True)\n",
      "        (1): LeakyReLU(negative_slope=0.01)\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "    (norm_context): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (0): Flatten(start_dim=1, end_dim=-1)\n",
      "  (1): Linear(in_features=8192, out_features=1024, bias=True)\n",
      "  (2): LeakyReLU(negative_slope=0.01)\n",
      "  (3): InstanceNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "  (4): Dropout(p=0.5, inplace=False)\n",
      "  (5): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (6): LeakyReLU(negative_slope=0.01)\n",
      "  (7): InstanceNorm1d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "  (8): Dropout(p=0.5, inplace=False)\n",
      "  (9): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (10): LeakyReLU(negative_slope=0.01)\n",
      "  (11): InstanceNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "  (12): Dropout(p=0.5, inplace=False)\n",
      "  (13): Linear(in_features=256, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'x'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[43], line 72\u001B[0m\n\u001B[1;32m     63\u001B[0m         \u001B[38;5;66;03m# final reconstruction\u001B[39;00m\n\u001B[1;32m     64\u001B[0m         \u001B[38;5;66;03m# print error vector\u001B[39;00m\n\u001B[1;32m     65\u001B[0m         \u001B[38;5;66;03m# print((omic - rec_omic).abs())\u001B[39;00m\n\u001B[1;32m     66\u001B[0m         \u001B[38;5;66;03m# print(omic)\u001B[39;00m\n\u001B[1;32m     67\u001B[0m         \u001B[38;5;66;03m# print(rec_omic)\u001B[39;00m\n\u001B[1;32m     68\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m model\n\u001B[0;32m---> 72\u001B[0m tab_model \u001B[38;5;241m=\u001B[39m \u001B[43mpretrain_loop\u001B[49m\u001B[43m(\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mblca\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     73\u001B[0m tab_latent \u001B[38;5;241m=\u001B[39m tab_model\u001B[38;5;241m.\u001B[39mget_latent()\n",
      "Cell \u001B[0;32mIn[43], line 42\u001B[0m, in \u001B[0;36mpretrain_loop\u001B[0;34m(data, batch_size, epochs)\u001B[0m\n\u001B[1;32m     40\u001B[0m [omic], censorship, event_time, y_disc \u001B[38;5;241m=\u001B[39m batch\n\u001B[1;32m     41\u001B[0m omic \u001B[38;5;241m=\u001B[39m omic\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m---> 42\u001B[0m rec_omic \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43momic\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     43\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m loss_method \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcontrastive\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m     44\u001B[0m     \u001B[38;5;66;03m# need to pass in larges for contrastive loss\u001B[39;00m\n\u001B[1;32m     45\u001B[0m     \u001B[38;5;66;03m# using torch.ones to ensure that omic and rec_omic are learned as similar representations\u001B[39;00m\n\u001B[1;32m     46\u001B[0m     \u001B[38;5;66;03m# note that this is a slight repurposing of the contrastive loss function\u001B[39;00m\n\u001B[1;32m     47\u001B[0m     \u001B[38;5;66;03m# with this, the loss is just 1-cos(omic, rec_omic)\u001B[39;00m\n\u001B[1;32m     48\u001B[0m     loss \u001B[38;5;241m=\u001B[39m loss_fn(input1\u001B[38;5;241m=\u001B[39momic, input2\u001B[38;5;241m=\u001B[39mrec_omic, target\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mones(omic\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m]))\n",
      "File \u001B[0;32m~/.conda/envs/cognition/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Cell \u001B[0;32mIn[41], line 57\u001B[0m, in \u001B[0;36mTabPretrainer.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     53\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlatent \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mParameter(einops\u001B[38;5;241m.\u001B[39mrepeat(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlatent, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mn d -> b n d\u001B[39m\u001B[38;5;124m\"\u001B[39m, b\u001B[38;5;241m=\u001B[39mb))\n\u001B[1;32m     55\u001B[0m \u001B[38;5;66;03m# encode\u001B[39;00m\n\u001B[1;32m     56\u001B[0m \u001B[38;5;66;03m# works much better with skip connections\u001B[39;00m\n\u001B[0;32m---> 57\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlatent\u001B[38;5;241m.\u001B[39mdata \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoder\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlatent\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlatent\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mdata\n\u001B[1;32m     58\u001B[0m rec_x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdecoder(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlatent)\n\u001B[1;32m     59\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m rec_x\n",
      "File \u001B[0;32m~/.conda/envs/cognition/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Cell \u001B[0;32mIn[39], line 52\u001B[0m, in \u001B[0;36mAttentionEncoder.forward\u001B[0;34m(self, x, latent)\u001B[0m\n\u001B[1;32m     42\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     43\u001B[0m \u001B[38;5;124;03mNote: context is the data, x is the latent\u001B[39;00m\n\u001B[1;32m     44\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     49\u001B[0m \n\u001B[1;32m     50\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     51\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m layer \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayers:\n\u001B[0;32m---> 52\u001B[0m     latent \u001B[38;5;241m=\u001B[39m \u001B[43mlayer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlatent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcontext\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     53\u001B[0m     \u001B[38;5;66;03m# # if isinstance(layer, LatentCrossAttention):\u001B[39;00m\n\u001B[1;32m     54\u001B[0m     \u001B[38;5;66;03m#     latent = layer(query=query, context=latent)\u001B[39;00m\n\u001B[1;32m     55\u001B[0m     \u001B[38;5;66;03m# else:\u001B[39;00m\n\u001B[1;32m     56\u001B[0m     \u001B[38;5;66;03m#     latent = layer(latent)\u001B[39;00m\n\u001B[1;32m     57\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m latent\n",
      "File \u001B[0;32m~/.conda/envs/cognition/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "\u001B[0;31mTypeError\u001B[0m: forward() missing 1 required positional argument: 'x'"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "\n",
    "\n",
    "def pretrain_loop(\n",
    "        data: TCGADataset,\n",
    "        batch_size: int, \n",
    "        epochs: int = 10,\n",
    "    ):\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    loader = DataLoader(\n",
    "        data, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True, \n",
    "        num_workers=multiprocessing.cpu_count()-1\n",
    "    )\n",
    "    [omic_sample], _, _, _ = next(iter(loader))\n",
    "    \n",
    "    \n",
    "    model = TabPretrainer(\n",
    "        sample = omic_sample, \n",
    "        input_axis=1, \n",
    "        latent_shape=[256, 32], # (l_n x l_d)\n",
    "        attn_dropout=0.1, \n",
    "        num_heads=8,\n",
    "        num_freq_bands=8\n",
    "    )\n",
    "    model.to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    \n",
    "    loss_method = \"mse\"\n",
    "    loss_fn = TabularLoss(method=loss_method)\n",
    "    \n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        for idx, batch in enumerate(loader):\n",
    "            [omic], censorship, event_time, y_disc = batch\n",
    "            omic = omic.to(device)\n",
    "            rec_omic = model(omic)\n",
    "            if loss_method == \"contrastive\":\n",
    "                # need to pass in larges for contrastive loss\n",
    "                # using torch.ones to ensure that omic and rec_omic are learned as similar representations\n",
    "                # note that this is a slight repurposing of the contrastive loss function\n",
    "                # with this, the loss is just 1-cos(omic, rec_omic)\n",
    "                loss = loss_fn(input1=omic, input2=rec_omic, target=torch.ones(omic.shape[0]))\n",
    "            elif loss_method == \"mse\": \n",
    "                loss = loss_fn(input=omic, target=rec_omic)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # print every 10th batch\n",
    "            if idx % 100 == 0:\n",
    "                pass\n",
    "                # print(loss)\n",
    "                # print(omic)\n",
    "                # print(rec_omic)\n",
    "        # print epoch-level stats\n",
    "        print(f\"Epoch {epoch+1} loss: {loss}\")\n",
    "        # final reconstruction\n",
    "        # print error vector\n",
    "        # print((omic - rec_omic).abs())\n",
    "        # print(omic)\n",
    "        # print(rec_omic)\n",
    "    return model\n",
    "        \n",
    "            \n",
    "    \n",
    "tab_model = pretrain_loop( data=blca, batch_size=1, epochs=5)\n",
    "tab_latent = tab_model.get_latent()\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-01T14:16:53.639226Z",
     "start_time": "2023-11-01T14:16:52.328412Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# get encoder attention weights for a test sample\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder = tab_model.encoder.to(device)\n",
    "encoder.eval()\n",
    "sample = next(iter(blca_loader))\n",
    "[omic], _, _, _ = sample\n",
    "omic = omic.to(device)\n",
    "# initialise latent\n",
    "latent = torch.randn(1, 256, 32).to(device)\n",
    "omic.to(device)\n",
    "latent.to(device)\n",
    "# watch out for leakage here\n",
    "# print(tab_latent)\n",
    "encoder(query=omic, latent=tab_latent)\n",
    "encoder.layers[0].attn_weights.shape\n",
    "encoder.layers[0].attn_weights[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-01T14:16:53.619109Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_attn_weight_distribution(attn_weights: torch.Tensor):\n",
    "    attn_weights = attn_weights.cpu().detach().numpy()\n",
    "    attn_weights = attn_weights.flatten()\n",
    "    sns.histplot(attn_weights)\n",
    "    plt.show()\n",
    "    \n",
    "def get_most_important_features(attn_weights: torch.Tensor, blca: TCGADataset):\n",
    "    attn_weights = attn_weights.cpu().detach().numpy()\n",
    "    attn_weights = attn_weights.flatten()\n",
    "    # get top 10 features\n",
    "    top_10 = np.argsort(attn_weights)[-10:]\n",
    "    # get feature names\n",
    "    feature_names = blca.omic_df.columns\n",
    "    top_10_features = feature_names[top_10]\n",
    "    return top_10_features\n",
    "    \n",
    "plot_attn_weight_distribution(encoder.layers[0].attn_weights[0])\n",
    "get_most_important_features(encoder.layers[0].attn_weights[0], blca)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-01T14:09:43.063170Z",
     "start_time": "2023-11-01T14:09:43.030910Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-01T14:09:43.031579Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-01T14:09:43.038933Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-01T14:09:43.081778Z",
     "start_time": "2023-11-01T14:09:43.078363Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-01T14:09:43.078618Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cognition",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
