{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T11:47:51.370453Z",
     "start_time": "2023-10-31T11:47:49.603615Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>.container { width:100% !important; }</style>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "if \"x_perceiver\" not in os.listdir():\n",
    "    os.chdir(\"/home/kh701/pycharm/healnet/\")\n",
    "import torch\n",
    "from torch import nn\n",
    "import multiprocessing\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "import einops\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from healnet.models.explainer import Explainer\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "\n",
    "from healnet.utils import Config, flatten_config\n",
    "from healnet.etl import TCGADataset\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "    \n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T11:48:01.505815Z",
     "start_time": "2023-10-31T11:47:51.307802Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filled 0 missing values with mean\n",
      "Missing values per feature: \n",
      " Series([], dtype: int64)\n",
      "Slides available: 436\n",
      "Omic available: 437\n",
      "Overlap: 436\n",
      "Filtering out 1 samples for which there are no omic data available\n",
      "Dataloader initialised for blca dataset\n",
      "Dataset: BLCA\n",
      "Molecular data shape: (436, 2191)\n",
      "Molecular/Slide match: 436/436\n",
      "Slide level count: 4\n",
      "Slide level dimensions: ((79968, 79653), (19992, 19913), (4998, 4978), (2499, 2489))\n",
      "Slide resize dimensions: w: 1024, h: 1024\n",
      "Sources selected: ['omic']\n",
      "Censored share: 0.539\n",
      "Survival_bin_sizes: {0: 72, 1: 83, 2: 109, 3: 172}\n",
      "Filled 0 missing values with mean\n",
      "Missing values per feature: \n",
      " Series([], dtype: int64)\n",
      "Slides available: 1019\n",
      "Omic available: 1022\n",
      "Overlap: 1019\n",
      "Filtering out 3 samples for which there are no omic data available\n",
      "Dataloader initialised for brca dataset\n",
      "Dataset: BRCA\n",
      "Molecular data shape: (1019, 2922)\n",
      "Molecular/Slide match: 1019/1019\n",
      "Slide level count: 3\n",
      "Slide level dimensions: ((35855, 34985), (8963, 8746), (2240, 2186))\n",
      "Slide resize dimensions: w: 1024, h: 1024\n",
      "Sources selected: ['omic']\n",
      "Censored share: 0.868\n",
      "Survival_bin_sizes: {3: 155, 2: 172, 1: 289, 0: 403}\n"
     ]
    }
   ],
   "source": [
    "# get dataloaders\n",
    "config = Config(\"config/main_gpu.yml\").read()\n",
    "config = flatten_config(config) # TODO - refactor to other \n",
    "\n",
    "blca = TCGADataset(\n",
    "    dataset=\"blca\", \n",
    "    config=config, \n",
    "    level=2, \n",
    "    sources=[\"omic\"]\n",
    ")\n",
    "\n",
    "brca = TCGADataset(\n",
    "    dataset=\"brca\", \n",
    "    config=config, \n",
    "    level=2, \n",
    "    sources=[\"omic\"]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "# get tabular data\n",
    "blca_loader = DataLoader(\n",
    "    blca, \n",
    "    batch_size=1, \n",
    "    shuffle=True, \n",
    "    num_workers=multiprocessing.cpu_count()-1\n",
    ")\n",
    "[sample], censorship, event_time, y_disc = next(iter(blca_loader))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T11:48:01.934939Z",
     "start_time": "2023-10-31T11:48:01.488900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 1, 2183])"
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T11:48:01.980750Z",
     "start_time": "2023-10-31T11:48:01.938695Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tabular self-supervised pre-training\n",
    "\n",
    "To start with, we want to build and encoder-decoder model which trains a cross-attention unit as the encoder, which can later on be deployed in the iterative model. We then want to benchmark the performance with pan-cancer pre-training vs. without pre-training. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "outputs": [],
   "source": [
    "from healnet.models.healnet import PreNorm, default, temperature_softmax, exists\n",
    "from einops import rearrange, repeat\n",
    "from torch import nn, einsum\n",
    "\n",
    "\n",
    "# class Attention(nn.Module):\n",
    "#     def __init__(self, query_dim, context_dim = None, heads = 8, dim_head = 64, dropout = 0.):\n",
    "#         super().__init__()\n",
    "#         inner_dim = dim_head * heads\n",
    "#         context_dim = default(context_dim, query_dim) # self-attention if context is not provided\n",
    "#         \n",
    "#         self.scale = dim_head ** -0.5\n",
    "#         self.heads = heads\n",
    "# \n",
    "#         self.to_q = nn.Linear(query_dim, inner_dim, bias = False)\n",
    "#         self.to_kv = nn.Linear(context_dim, inner_dim * 2, bias = False)\n",
    "# \n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "#         # add leaky relu\n",
    "#         self.to_out = nn.Sequential(\n",
    "#             nn.Linear(inner_dim, context_dim),\n",
    "#             nn.LeakyReLU(negative_slope=1e-2)\n",
    "#         )\n",
    "# \n",
    "#         self.attn_weights = None\n",
    "#         # self._init_weights()\n",
    "# \n",
    "#     def _init_weights(self):\n",
    "#     # Use He initialization for Linear layers\n",
    "#         for m in self.modules():\n",
    "#             if isinstance(m, nn.Linear):\n",
    "#                 nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "#                 # Initialize bias to zero if there's any\n",
    "#                 if m.bias is not None:\n",
    "#                     nn.init.zeros_(m.bias)\n",
    "# \n",
    "#     def forward(self, x, context = None, mask = None):\n",
    "#         h = self.heads\n",
    "#         print(x.shape)\n",
    "#         print(context.shape)\n",
    "# \n",
    "#         q = self.to_q(x)\n",
    "#         context = default(context, x)\n",
    "#         k, v = self.to_kv(context).chunk(2, dim = -1)\n",
    "# \n",
    "#         q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h = h), (q, k, v))\n",
    "# \n",
    "#         sim = einsum('b i d, b j d -> b i j', q, k) * self.scale\n",
    "# \n",
    "#         if exists(mask):\n",
    "#             mask = rearrange(mask, 'b ... -> b (...)')\n",
    "#             max_neg_value = -torch.finfo(sim.dtype).max\n",
    "#             mask = repeat(mask, 'b j -> (b h) () j', h = h)\n",
    "#             sim.masked_fill_(~mask, max_neg_value)\n",
    "# \n",
    "#         # attention, what we cannot get enough of\n",
    "#         # attn = sim.softmax(dim = -1)\n",
    "#         attn = temperature_softmax(sim, temperature=0.5, dim=-1)\n",
    "#         self.attn_weights = attn\n",
    "#         print(\"Attn weights\", self.attn_weights.shape)\n",
    "#         attn = self.dropout(attn)\n",
    "# \n",
    "# \n",
    "#         out = einsum('b i j, b j d -> b i d', attn, v)\n",
    "#         out = rearrange(out, '(b h) n d -> b n (h d)', h = h)\n",
    "#         out = self.to_out(out)\n",
    "#         return out"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T12:13:34.826262Z",
     "start_time": "2023-10-31T12:13:34.730517Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T12:13:34.999814Z",
     "start_time": "2023-10-31T12:13:34.927203Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (10x2189 and 1x32)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[209], line 57\u001B[0m\n\u001B[1;32m     54\u001B[0m context \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mrandn(b, num_latents, latent_dim)\n\u001B[1;32m     56\u001B[0m attention_module \u001B[38;5;241m=\u001B[39m LatentCrossAttention(query_dim\u001B[38;5;241m=\u001B[39minput_channels, latent_dim\u001B[38;5;241m=\u001B[39mlatent_dim)\n\u001B[0;32m---> 57\u001B[0m context_prime \u001B[38;5;241m=\u001B[39m \u001B[43mattention_module\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcontext\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     59\u001B[0m \u001B[38;5;66;03m# print(attention_weights.shape)  # Expected: torch.Size([64, 2189])\u001B[39;00m\n\u001B[1;32m     60\u001B[0m \u001B[38;5;66;03m# print(query_prime.shape)\u001B[39;00m\n\u001B[1;32m     61\u001B[0m \u001B[38;5;28mprint\u001B[39m(context_prime\u001B[38;5;241m.\u001B[39mshape)\n",
      "File \u001B[0;32m~/.conda/envs/cognition/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Cell \u001B[0;32mIn[209], line 19\u001B[0m, in \u001B[0;36mLatentCrossAttention.forward\u001B[0;34m(self, query, context)\u001B[0m\n\u001B[1;32m     17\u001B[0m query \u001B[38;5;241m=\u001B[39m einops\u001B[38;5;241m.\u001B[39mrearrange(query, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb c d -> b d c\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     18\u001B[0m \u001B[38;5;66;03m# Linear projections of query and context\u001B[39;00m\n\u001B[0;32m---> 19\u001B[0m q_proj \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mw_q\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# [b, 2189, 32]\u001B[39;00m\n\u001B[1;32m     20\u001B[0m k_proj \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mw_c(context)  \u001B[38;5;66;03m# [b, 256, 32]\u001B[39;00m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;66;03m# Calculating attention scores\u001B[39;00m\n",
      "File \u001B[0;32m~/.conda/envs/cognition/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/.conda/envs/cognition/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001B[0m, in \u001B[0;36mLinear.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 114\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: mat1 and mat2 shapes cannot be multiplied (10x2189 and 1x32)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LatentCrossAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Module which takes in a query tensor and a context tensor, and returns an updated context tensor.\n",
    "    \"\"\"\n",
    "    def __init__(self, query_dim=1, latent_dim=32):\n",
    "        super(LatentCrossAttention, self).__init__()\n",
    "\n",
    "        # Weight matrices for projecting query and context (key)\n",
    "        self.w_q = nn.Linear(query_dim, latent_dim, bias=False)  # Linear layer will act as our transformation matrix for Q\n",
    "        self.w_c = nn.Linear(latent_dim, latent_dim, bias=False)    # Transformation matrix for K (context)\n",
    "\n",
    "    def forward(self, query, context):\n",
    "        query = einops.rearrange(query, \"b c d -> b d c\")\n",
    "        # Linear projections of query and context\n",
    "        q_proj = self.w_q(query)  # [b, 2189, 32]\n",
    "        k_proj = self.w_c(context)  # [b, 256, 32]\n",
    "\n",
    "        # Calculating attention scores\n",
    "        S = torch.bmm(q_proj, k_proj.transpose(1, 2))  # [b, 2189, 256]\n",
    "\n",
    "        # Summing over the second dimension to get the required size\n",
    "        S_mean = torch.mean(S, dim=-1)  # [b, 2189]\n",
    "\n",
    "        # Calculating attention weights\n",
    "        attn = F.softmax(S_mean, dim=-1)  # [b, 2189]\n",
    "\n",
    "        # Compute the attention-weighted sum of q_proj\n",
    "        weighted_q_proj = torch.sum(attn.unsqueeze(-1) * q_proj, dim=1, keepdim=True)  # [b, 1, 32]\n",
    "\n",
    "        # Adding the update to the original context to get the updated context\n",
    "        context_prime = F.softmax(context + (weighted_q_proj * k_proj), dim=-1)\n",
    "\n",
    "        # element-wise (hadamard) product\n",
    "        query_prime = attn.unsqueeze(-1) * query  # [b, 2189, 1]\n",
    "\n",
    "        self.attn_weights = attn\n",
    "        self.query_prime = query_prime\n",
    "\n",
    "        return context_prime\n",
    "\n",
    "# Test case\n",
    "b = 10\n",
    "# tabular\n",
    "channel_dims = 2189\n",
    "input_channels = 1 # just one channel for tabular\n",
    "num_latents = 256\n",
    "latent_dim = 32\n",
    "# latent_dim\n",
    "query = torch.randn(b, channel_dims, input_channels)\n",
    "context = torch.randn(b, num_latents, latent_dim)\n",
    "\n",
    "attention_module = LatentCrossAttention(query_dim=input_channels, latent_dim=latent_dim)\n",
    "context_prime = attention_module(query, context)\n",
    "\n",
    "# print(attention_weights.shape)  # Expected: torch.Size([64, 2189])\n",
    "# print(query_prime.shape)\n",
    "print(context_prime.shape)\n",
    "\n",
    "\n",
    "# latent_update = LatentUpdate()\n",
    "# context_prime = latent_update(context, query_prime)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T12:56:15.047710Z",
     "start_time": "2023-10-31T12:56:14.939997Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 62\n",
      "torch.Size([80, 1, 32]) torch.Size([80, 256, 32]) torch.Size([80, 256, 32])\n"
     ]
    },
    {
     "data": {
      "text/plain": "torch.Size([10, 256, 62])"
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class AttentionUpdate(nn.Module):\n",
    "    def __init__(self, query_dim, latent_dim = None, heads = 8, dim_head = 64, dropout = 0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        context_dim = default(latent_dim, query_dim)\n",
    "\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.heads = heads\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        self.to_q = nn.Linear(query_dim, inner_dim, bias = False)\n",
    "        self.to_kv = nn.Linear(context_dim, inner_dim * 2, bias = False)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.attn_weights = None\n",
    "        # self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "    # Use He initialization for Linear layers\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "                # Initialize bias to zero if there's any\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, context, x, mask = None):\n",
    "        h = self.heads\n",
    "        # x = einops.rearrange(x, \"b d c -> b c d\")\n",
    "\n",
    "        q = self.to_q(x)\n",
    "        context = default(context, x)\n",
    "        k, v = self.to_kv(context).chunk(2, dim = -1)\n",
    "\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h = h), (q, k, v))\n",
    "        print(q.shape, k.shape, v.shape) \n",
    "\n",
    "        sim = einsum('b i d, b j d -> b i j', q, k) * self.scale\n",
    "\n",
    "        if exists(mask):\n",
    "            mask = rearrange(mask, 'b ... -> b (...)')\n",
    "            max_neg_value = -torch.finfo(sim.dtype).max\n",
    "            mask = repeat(mask, 'b j -> (b h) () j', h = h)\n",
    "            sim.masked_fill_(~mask, max_neg_value)\n",
    "\n",
    "        # attention, what we cannot get enough of\n",
    "        # attn = sim.softmax(dim = -1)\n",
    "        attn = temperature_softmax(sim, temperature=0.5, dim=-1)\n",
    "        attn = einops.rearrange(attn, '(b h) n j -> b h n j', h=h)\n",
    "        self.attn_weights = attn\n",
    "        attn = self.dropout(attn)\n",
    "        attn = attn.mean(dim=-1).mean(dim=1)\n",
    "\n",
    "        nn.LeakyReLU(negative_slope=1e-2)\n",
    "        to_out = nn.Sequential(\n",
    "            nn.Linear(attn.shape[1], self.latent_dim),\n",
    "            nn.LeakyReLU(negative_slope=1e-2))\n",
    "        # to_out.to(\"cuda\")\n",
    "\n",
    "        out = to_out(attn)\n",
    "        # element-wise product\n",
    "        out = einsum('b d, b i d -> b i d', out, context)\n",
    "        return out\n",
    "\n",
    "b = 10 \n",
    "# tabular\n",
    "channel_dims = 2189\n",
    "input_channels = 1 # just one channel for tabular\n",
    "num_latents = 256\n",
    "latent_dim = 62\n",
    "latent = torch.randn(b, num_latents, latent_dim)\n",
    "query = torch.randn(b, input_channels, channel_dims)\n",
    "\n",
    "print(input_channels, latent_dim)\n",
    "attention_module = AttentionUpdate(query_dim=channel_dims, latent_dim=latent_dim, heads=8, dim_head=32, dropout=0.1)\n",
    "out = attention_module(context=latent, x=query)\n",
    "out.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T16:40:41.250824Z",
     "start_time": "2023-10-31T16:40:37.578703Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([10, 2189, 1])"
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T15:27:53.993399Z",
     "start_time": "2023-10-31T15:27:53.938427Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([10, 2189, 32, 1])"
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(10, 2189, 32)\n",
    "b = torch.randn(10, 256, 32)\n",
    "# expand dim\n",
    "a = einops.repeat(a, \"b n d -> b n d j\", j=1)\n",
    "b = einops.repeat(b, \"b n d -> b n d j\", j=2189)\n",
    "\n",
    "# torch.bmm(a, b.transpose(1, 2)).shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T14:08:49.995651Z",
     "start_time": "2023-10-31T14:08:49.938882Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T12:13:35.280365Z",
     "start_time": "2023-10-31T12:13:35.179786Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "outputs": [],
   "source": [
    "\n",
    "class AttentionEncoder(nn.Module): \n",
    "    \"\"\"\n",
    "    Simple encoder that uses fourier encoding, pre-norm and cross-attention to encode the input features into a latent array \n",
    "    of size (num_latents x latent_dim). Takes in both the input tensors as well as a randomly initialised latent \n",
    "    array as the input. \n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 input_channels: int,\n",
    "                 channel_dims: int,\n",
    "                 latent: torch.Tensor,\n",
    "                 input_axis: int = 1,\n",
    "                 attn_dropout: float = 0.1,\n",
    "                 num_heads: int = 4,\n",
    "                 num_freq_bands: int=8,\n",
    "                 ):    \n",
    "        super().__init__()\n",
    "        \n",
    "        self.channel_dims = channel_dims\n",
    "        self.input_axis = input_axis\n",
    "        self.attn_dropout = attn_dropout\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        \n",
    "        # fourier_channels = (input_axis * ((num_freq_bands * 2) + 1))\n",
    "        # input_dim = fourier_channels + input_channels\n",
    "        input_dim = channel_dims * input_channels\n",
    "        \n",
    "        num_latents, latent_dim = latent.shape\n",
    "        print(latent_dim, input_dim)\n",
    "        # enc = PreNorm(latent_dim, Attention(latent_dim, input_dim, heads=num_heads, dim_head=num_heads, dropout=attn_dropout), context_dim=latent_dim)\n",
    "        # enc = Attention(query_dim=input_dim, num_latents=num_latents, latent_dim=latent_dim, heads=num_heads, dim_head=num_heads, dropout=attn_dropout)\n",
    "        print(f\"Input channels {input_channels}, latent dim {latent_dim}\")\n",
    "        \n",
    "        attn = LatentCrossAttention(query_dim=input_channels, latent_dim=latent_dim)\n",
    "        norm = nn.InstanceNorm1d(latent_dim)\n",
    "        # enc = PreNorm\n",
    "        # enc = PreNorm(latent_dim, Attention(query_dim=input_dim, context_dim=latent_dim, heads=num_heads, dim_head=num_heads, dropout=attn_dropout), context_dim=latent_dim)\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            attn, \n",
    "            norm\n",
    "        ])\n",
    "        \n",
    "        print(self.layers)\n",
    "        \n",
    "    def forward(self, query: torch.Tensor, latent: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Note: context is the data, x is the latent\n",
    "        Args:\n",
    "            latent: \n",
    "            context: \n",
    "\n",
    "        Returns:\n",
    "\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, LatentCrossAttention):\n",
    "                latent = layer(query=query, context=latent)\n",
    "            else:\n",
    "                latent = layer(latent)\n",
    "        return latent\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T12:16:56.576772Z",
     "start_time": "2023-10-31T12:16:56.483818Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The decoder often needs to be different depending on the modality, so let's implement modality-specific decoders while trying to have a relatively general-purpose encoder that we can plug into the pipeline.\n",
    "\n",
    "Note that we may change this later down the line. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-31T12:16:06.051995Z",
     "start_time": "2023-10-31T12:16:05.995041Z"
    }
   },
   "outputs": [],
   "source": [
    "class TabularDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder suited for tabular data. We use the following: \n",
    "    - Skip connections: faster and more stable training\n",
    "    - Batch normalisation: stabilises the activations and speeds up training\n",
    "    - Activation: Output layer to map back to output dimensions, corresponding to the original data dims\n",
    "    Tries to reconstruct the original input given the latent\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_dim: int, num_latents: int, output_dim: int, method: str = \"dense\"):\n",
    "        super(TabularDecoder, self).__init__()\n",
    "        assert method in [\"dense\", \"conv\"], \"Decoder type not recognised\"\n",
    "        # check that latent_dim is divisible by 4\n",
    "        assert num_latents % 4 == 0, \"Latent dim must be a multiple of 4\"\n",
    "        layers = []\n",
    "        \n",
    "        if method == \"dense\": \n",
    "            \n",
    "            # flatten latent array (batch, num_latents, latent_dim) -> (batch, num_latents * latent_dim)\n",
    "            layers.extend([nn.Flatten()]) \n",
    "            out_dims = [1024, 512, 256] # may refactor as hyperparameter later\n",
    "            \n",
    "            in_dim = latent_dim * num_latents\n",
    "            for idx, out_dim in enumerate(out_dims):\n",
    "                \n",
    "                layers.extend([\n",
    "                    nn.Linear(in_features=in_dim, out_features=out_dim), \n",
    "                    nn.LeakyReLU(), \n",
    "                    nn.InstanceNorm1d(out_dim, track_running_stats=False), \n",
    "                    nn.Dropout(0.5)\n",
    "                ])\n",
    "                \n",
    "                in_dim = out_dim # update for next layer\n",
    "            \n",
    "            # final layer to reconstruct output\n",
    "            layers.append(nn.Linear(in_dim, output_dim))\n",
    "        \n",
    "        elif method == \"conv\": \n",
    "            print(latent_dim, num_latents)\n",
    "            layers.extend([\n",
    "                nn.ConvTranspose1d(num_latents, out_channels=int(num_latents/2), kernel_size=4, stride=2, padding=1), \n",
    "                nn.BatchNorm1d(int(num_latents/2)),\n",
    "                nn.LeakyReLU(negative_slope=0.1),\n",
    "                \n",
    "                nn.ConvTranspose1d(int(num_latents/2), out_channels=int(num_latents/4), kernel_size=4, stride=2, padding=1),\n",
    "                nn.BatchNorm1d(int(num_latents/4)),\n",
    "                nn.LeakyReLU(negative_slope=0.1),\n",
    "                \n",
    "                # If you added any other ConvTranspose layers, ensure the channel sizes match correctly for those as well.\n",
    "                \n",
    "                nn.Conv1d(int(num_latents/4), out_channels=1, kernel_size=1, stride=1, padding=0)\n",
    "            ])\n",
    "        \n",
    "        self.decode = nn.Sequential(*layers)\n",
    "        print(self.decode)\n",
    "        \n",
    "    def forward(self, latent: torch.Tensor):\n",
    "        return self.decode(latent)\n",
    "    \n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, putting it all together in the encoder-decoder model\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "outputs": [],
   "source": [
    "from typing import *\n",
    "\n",
    "class TabPretrainer(nn.Module): \n",
    "    \"\"\"\n",
    "    Encoder-decoder model for pre-training tabular data.\n",
    "    # TODO - refactor abstract base class for initialisations \n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 sample: torch.Tensor,\n",
    "                 latent_shape: List[int],\n",
    "                 input_axis: int = 1,\n",
    "                 attn_dropout: float = 0.1,\n",
    "                 num_heads: int = 4,\n",
    "                 num_freq_bands: int=8,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.input_channels = sample.shape[-2]\n",
    "        print(sample.shape)\n",
    "        self.channel_dims = sample.shape[-1]\n",
    "        self.input_axis = input_axis\n",
    "        self.num_latents, self.latent_dim = latent_shape  # (n x d) [256, 32]\n",
    "        self.attn_dropout = attn_dropout\n",
    "        self.num_heads = num_heads\n",
    "        self.num_freq_bands = num_freq_bands\n",
    "        \n",
    "        \n",
    "        # randomly initialise latent\n",
    "        self.latent = nn.Parameter(torch.randn(self.num_latents, self.latent_dim))\n",
    "        \n",
    "        # encoder\n",
    "        self.encoder = AttentionEncoder(\n",
    "            input_channels=self.input_channels, \n",
    "            channel_dims=self.channel_dims,\n",
    "            latent=self.latent,\n",
    "            input_axis=self.input_axis, \n",
    "            attn_dropout=attn_dropout, \n",
    "            num_heads=num_heads, \n",
    "            num_freq_bands=num_freq_bands\n",
    "        )\n",
    "        \n",
    "        # decoder\n",
    "        self.decoder = TabularDecoder(\n",
    "            latent_dim=self.latent_dim,\n",
    "            num_latents=self.num_latents,\n",
    "            output_dim=self.input_channels,\n",
    "            method=\"dense\" # using simple encoder to force good representation\n",
    "        )\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \n",
    "        # expand latent to batch size\n",
    "        if len(self.latent.shape) == 2:\n",
    "            b = x.shape[0]\n",
    "            self.latent = nn.Parameter(einops.repeat(self.latent, \"n d -> b n d\", b=b))\n",
    "        \n",
    "        # encode\n",
    "        # works much better with skip connections\n",
    "        self.latent.data = self.encoder(query=x, latent=self.latent).data\n",
    "        rec_x = self.decoder(self.latent)\n",
    "        return rec_x\n",
    "    \n",
    "    def get_latent(self):\n",
    "        return self.latent"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T12:57:27.679498Z",
     "start_time": "2023-10-31T12:57:27.625647Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we need to think about tabular loss functions. Here, we can explore both reconstruction losses and contrastive losses. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "outputs": [],
   "source": [
    "class TabularLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Reconstruction loss functions for tabular data. We use two types which are commonly used with continuous data: \n",
    "    - Mean squared error\n",
    "    - Constrastive loss, measured as cosine distance between the original and reconstructed data\n",
    "    We seek to minimise both objectives.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 method: str = \"mse\",\n",
    "                 reduction: str = \"mean\",\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        assert method in [\"mse\", \"contrastive\"], \"Loss type not recognised\"\n",
    "        self.loss_type = method\n",
    "        self.reduction = reduction\n",
    "        \n",
    "        if method == \"mse\":\n",
    "            self.loss = nn.MSELoss(reduction=reduction)\n",
    "        elif method == \"contrastive\":\n",
    "            self.loss = nn.CosineEmbeddingLoss(reduction=reduction)\n",
    "            \n",
    "    def __call__(self, **kwargs):\n",
    "        return self.loss(**kwargs)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T12:16:06.587854Z",
     "start_time": "2023-10-31T12:16:06.510786Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, we write a pre-training loop that we can use for pre-training across cancer sites. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 2183])\n",
      "32 2183\n",
      "Input channels 1, latent dim 32\n",
      "ModuleList(\n",
      "  (0): LatentCrossAttention(\n",
      "    (w_q): Linear(in_features=1, out_features=32, bias=False)\n",
      "    (w_c): Linear(in_features=32, out_features=32, bias=False)\n",
      "  )\n",
      "  (1): InstanceNorm1d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      ")\n",
      "Sequential(\n",
      "  (0): Flatten(start_dim=1, end_dim=-1)\n",
      "  (1): Linear(in_features=8192, out_features=1024, bias=True)\n",
      "  (2): LeakyReLU(negative_slope=0.01)\n",
      "  (3): InstanceNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "  (4): Dropout(p=0.5, inplace=False)\n",
      "  (5): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (6): LeakyReLU(negative_slope=0.01)\n",
      "  (7): InstanceNorm1d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "  (8): Dropout(p=0.5, inplace=False)\n",
      "  (9): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (10): LeakyReLU(negative_slope=0.01)\n",
      "  (11): InstanceNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "  (12): Dropout(p=0.5, inplace=False)\n",
      "  (13): Linear(in_features=256, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]/home/kh701/.conda/envs/cognition/lib/python3.9/site-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1, 1, 2183])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      " 20%|██        | 1/5 [00:02<00:10,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss: 3.017691135406494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:05<00:07,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 loss: 2.1813321113586426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [00:07<00:05,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 loss: 2.835190534591675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [00:10<00:02,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 loss: 4.7551984786987305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:12<00:00,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 loss: 2.494621992111206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "\n",
    "\n",
    "def pretrain_loop(\n",
    "        data: TCGADataset,\n",
    "        batch_size: int, \n",
    "        epochs: int = 10,\n",
    "    ):\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    loader = DataLoader(\n",
    "        data, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True, \n",
    "        num_workers=multiprocessing.cpu_count()-1\n",
    "    )\n",
    "    [omic_sample], _, _, _ = next(iter(loader))\n",
    "    \n",
    "    \n",
    "    model = TabPretrainer(\n",
    "        sample = omic_sample, \n",
    "        input_axis=1, \n",
    "        latent_shape=[256, 32], # (n_l x d_l)\n",
    "        attn_dropout=0.1, \n",
    "        num_heads=8,\n",
    "        num_freq_bands=8\n",
    "    )\n",
    "    model.to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    \n",
    "    loss_method = \"mse\"\n",
    "    loss_fn = TabularLoss(method=loss_method)\n",
    "    \n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        for idx, batch in enumerate(loader):\n",
    "            [omic], censorship, event_time, y_disc = batch\n",
    "            omic = omic.to(device)\n",
    "            rec_omic = model(omic)\n",
    "            if loss_method == \"contrastive\":\n",
    "                # need to pass in larges for contrastive loss\n",
    "                # using torch.ones to ensure that omic and rec_omic are learned as similar representations\n",
    "                # note that this is a slight repurposing of the contrastive loss function\n",
    "                # with this, the loss is just 1-cos(omic, rec_omic)\n",
    "                loss = loss_fn(input1=omic, input2=rec_omic, target=torch.ones(omic.shape[0]))\n",
    "            elif loss_method == \"mse\": \n",
    "                loss = loss_fn(input=omic, target=rec_omic)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # print every 10th batch\n",
    "            if idx % 100 == 0:\n",
    "                pass\n",
    "                # print(loss)\n",
    "                # print(omic)\n",
    "                # print(rec_omic)\n",
    "        # print epoch-level stats\n",
    "        print(f\"Epoch {epoch+1} loss: {loss}\")\n",
    "        # final reconstruction\n",
    "        # print error vector\n",
    "        # print((omic - rec_omic).abs())\n",
    "        # print(omic)\n",
    "        # print(rec_omic)\n",
    "    return model\n",
    "        \n",
    "            \n",
    "    \n",
    "tab_model = pretrain_loop( data=blca, batch_size=1, epochs=5)\n",
    "tab_latent = tab_model.get_latent()\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T12:57:46.275230Z",
     "start_time": "2023-10-31T12:57:32.819614Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([    0.0000,     0.0005,     0.0005,  ...,     0.0005,     0.0005,\n            0.0005], device='cuda:0', grad_fn=<SelectBackward0>)"
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get encoder attention weights for a test sample\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder = tab_model.encoder.to(device)\n",
    "encoder.eval()\n",
    "sample = next(iter(blca_loader))\n",
    "[omic], _, _, _ = sample\n",
    "omic = omic.to(device)\n",
    "# initialise latent\n",
    "latent = torch.randn(1, 256, 32).to(device)\n",
    "omic.to(device)\n",
    "latent.to(device)\n",
    "# watch out for leakage here\n",
    "# print(tab_latent)\n",
    "encoder(query=omic, latent=tab_latent)\n",
    "encoder.layers[0].attn_weights.shape\n",
    "encoder.layers[0].attn_weights[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T12:42:32.615420Z",
     "start_time": "2023-10-31T12:42:31.970038Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Index(['PKN3_rnaseq', 'SLC44A1_rnaseq', 'OXT_rnaseq', 'CAMKV_rnaseq',\n       'CD86_rnaseq', 'MAPK3_rnaseq', 'MAP4K2_rnaseq', 'SEMA4G_rnaseq',\n       'PAX3_rnaseq', 'EBI3_rnaseq'],\n      dtype='object')"
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_attn_weight_distribution(attn_weights: torch.Tensor):\n",
    "    attn_weights = attn_weights.cpu().detach().numpy()\n",
    "    attn_weights = attn_weights.flatten()\n",
    "    sns.histplot(attn_weights)\n",
    "    plt.show()\n",
    "    \n",
    "def get_most_important_features(attn_weights: torch.Tensor, blca: TCGADataset):\n",
    "    attn_weights = attn_weights.cpu().detach().numpy()\n",
    "    attn_weights = attn_weights.flatten()\n",
    "    # get top 10 features\n",
    "    top_10 = np.argsort(attn_weights)[-10:]\n",
    "    # get feature names\n",
    "    feature_names = blca.omic_df.columns\n",
    "    top_10_features = feature_names[top_10]\n",
    "    return top_10_features\n",
    "    \n",
    "plot_attn_weight_distribution(encoder.layers[0].attn_weights[0])\n",
    "get_most_important_features(encoder.layers[0].attn_weights[0], blca)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-31T12:46:55.533685Z",
     "start_time": "2023-10-31T12:46:55.294324Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T10:29:47.823051Z",
     "start_time": "2023-10-30T10:29:47.818526Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T10:29:47.837352Z",
     "start_time": "2023-10-30T10:29:47.820303Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T10:29:47.839370Z",
     "start_time": "2023-10-30T10:29:47.824501Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cognition",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
