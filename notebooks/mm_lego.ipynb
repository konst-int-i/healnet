{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kh701/.conda/envs/cognition/bin/python3\r\n"
     ]
    }
   ],
   "source": [
    "! which python3"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T10:22:28.550493Z",
     "start_time": "2023-10-13T10:22:28.387505Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-13T10:59:54.511324Z",
     "start_time": "2023-10-13T10:59:54.423289Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>.container { width:100% !important; }</style>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "if \"x_perceiver\" not in os.listdir():\n",
    "    os.chdir(\"/home/kh701/pycharm/healnet/\")\n",
    "import torch\n",
    "from torch import nn\n",
    "import multiprocessing\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "import einops\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from healnet.models.explainer import Explainer\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "\n",
    "from healnet.utils import Config, flatten_config\n",
    "from healnet.etl import TCGADataset\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "    \n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-13T11:01:20.754077Z",
     "start_time": "2023-10-13T11:01:13.711632Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filled 0 missing values with mean\n",
      "Missing values per feature: \n",
      " Series([], dtype: int64)\n",
      "Slides available: 436\n",
      "Omic available: 437\n",
      "Overlap: 436\n",
      "Filtering out 1 samples for which there are no omic data available\n",
      "Dataloader initialised for blca dataset\n",
      "Dataset: BLCA\n",
      "Molecular data shape: (436, 2191)\n",
      "Molecular/Slide match: 436/436\n",
      "Slide level count: 4\n",
      "Slide level dimensions: ((79968, 79653), (19992, 19913), (4998, 4978), (2499, 2489))\n",
      "Slide resize dimensions: w: 1024, h: 1024\n",
      "Sources selected: ['omic']\n",
      "Censored share: 0.539\n",
      "Survival_bin_sizes: {0: 72, 1: 83, 2: 109, 3: 172}\n",
      "Filled 0 missing values with mean\n",
      "Missing values per feature: \n",
      " Series([], dtype: int64)\n",
      "Slides available: 1019\n",
      "Omic available: 1022\n",
      "Overlap: 1019\n",
      "Filtering out 3 samples for which there are no omic data available\n",
      "Dataloader initialised for brca dataset\n",
      "Dataset: BRCA\n",
      "Molecular data shape: (1019, 2922)\n",
      "Molecular/Slide match: 1019/1019\n",
      "Slide level count: 3\n",
      "Slide level dimensions: ((35855, 34985), (8963, 8746), (2240, 2186))\n",
      "Slide resize dimensions: w: 1024, h: 1024\n",
      "Sources selected: ['omic']\n",
      "Censored share: 0.868\n",
      "Survival_bin_sizes: {3: 155, 2: 172, 1: 289, 0: 403}\n"
     ]
    }
   ],
   "source": [
    "# get dataloaders\n",
    "config = Config(\"config/main_gpu.yml\").read()\n",
    "config = flatten_config(config) # TODO - refactor to other \n",
    "\n",
    "blca = TCGADataset(\n",
    "    dataset=\"blca\", \n",
    "    config=config, \n",
    "    level=2, \n",
    "    sources=[\"omic\"]\n",
    ")\n",
    "\n",
    "brca = TCGADataset(\n",
    "    dataset=\"brca\", \n",
    "    config=config, \n",
    "    level=2, \n",
    "    sources=[\"omic\"]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# get tabular data\n",
    "blca_loader = DataLoader(\n",
    "    blca, \n",
    "    batch_size=1, \n",
    "    shuffle=True, \n",
    "    num_workers=multiprocessing.cpu_count()-1\n",
    ")\n",
    "[blca_omic], censorship, event_time, y_disc = next(iter(blca_loader))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T11:02:10.462674Z",
     "start_time": "2023-10-13T11:02:09.846546Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 1, 2183])"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blca_omic.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T11:02:17.457800Z",
     "start_time": "2023-10-13T11:02:17.393059Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tabular self-supervised pre-training\n",
    "\n",
    "To start with, we want to build and encoder-decoder model which trains a cross-attention unit as the encoder, which can later on be deployed in the iterative model. We then want to benchmark the performance with pan-cancer pre-training vs. without pre-training. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "from healnet.models.healnet import Attention, PreNorm\n",
    "\n",
    "class AttentionEncoder(nn.Module): \n",
    "    \"\"\"\n",
    "    Simple encoder that uses fourier encoding, pre-norm and cross-attention to encode the input features into a latent array \n",
    "    of size (num_latents x latent_dim). Takes in both the input tensors as well as a randomly initialised latent \n",
    "    array as the input. \n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 input_channels: int,\n",
    "                 latent: torch.Tensor, \n",
    "                 input_axis: int = 1, \n",
    "                 attn_dropout: float = 0.1,\n",
    "                 num_heads: int = 4, \n",
    "                 num_freq_bands: int=8, \n",
    "                 ):    \n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_channels = input_channels\n",
    "        self.input_axis = input_axis\n",
    "        # self.num_latents = num_latents\n",
    "        # self.latent_dims = latent_dims\n",
    "        self.attn_dropout = attn_dropout\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        \n",
    "        fourier_channels = (input_axis * ((num_freq_bands * 2) + 1))\n",
    "        input_dim = fourier_channels + input_channels\n",
    "        \n",
    "        \n",
    "        latent_dim = latent.shape[-1] # required for PreNorm layer\n",
    "        # simple single attention unit\n",
    "        print(latent_dim, input_dim, num_heads, attn_dropout)\n",
    "        enc = PreNorm(latent_dim, Attention(latent_dim, input_dim, heads=num_heads, dim_head=num_heads, dropout=attn_dropout), context_dim=input_dim)\n",
    "        \n",
    "        self.layers = nn.ModuleList([enc])\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, latent: torch.Tensor):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x=x, context=latent)\n",
    "        return x, latent\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T15:14:13.707856Z",
     "start_time": "2023-10-13T15:14:13.636292Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The decoder often needs to be different depending on the modality, so let's implement modality-specific decoders while trying to have a relatively general-purpose encoder that we can plug into the pipeline.\n",
    "\n",
    "Note that we may change this later down the line. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-13T14:21:03.861337Z",
     "start_time": "2023-10-13T14:21:03.794348Z"
    }
   },
   "outputs": [],
   "source": [
    "class TabularDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder suited for tabular data. We use the following: \n",
    "    - Skip connections: faster and more stable training\n",
    "    - Batch normalisation: stabilises the activations and speeds up training\n",
    "    - Activation: Output layer to map back to output dimensions, corresponding to the original data dims\n",
    "    Tries to reconstruct the original input given the latent\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_dim: int, output_dim: int):\n",
    "        super(TabularDecoder, self).__init__()\n",
    "        \n",
    "        layers = []\n",
    "        hidden_dims = [128, 256] # may refactor as hyperparameter later\n",
    "        \n",
    "        for hidden_dim in hidden_dims: \n",
    "            layers.extend([\n",
    "                nn.Linear(latent_dim, hidden_dim), \n",
    "                nn.LeakyReLU(), \n",
    "                nn.BatchNorm1d(hidden_dim), \n",
    "                nn.Dropout(0.5)\n",
    "            ])\n",
    "            \n",
    "            latent_dim = hidden_dim # update for next layer\n",
    "        \n",
    "        # final layer to reconstruct output\n",
    "        layers.append(nn.Linear(latent_dim, output_dim))\n",
    "        \n",
    "        self.decode = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, latent: torch.Tensor):\n",
    "        return self.decode(latent)\n",
    "    \n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, putting it all together in the encoder-decoder model\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "from typing import *\n",
    "\n",
    "class TabPretrainer(nn.Module): \n",
    "    \"\"\"\n",
    "    Encoder-decoder model for pre-training tabular data.\n",
    "    # TODO - refactor abstract base class for initialisations \n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 input_channels: int,\n",
    "                 latent_shape: List[int],\n",
    "                 input_axis: int = 1,\n",
    "                 attn_dropout: float = 0.1,\n",
    "                 num_heads: int = 4,\n",
    "                 num_freq_bands: int=8,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.input_channels = input_channels\n",
    "        self.input_axis = input_axis\n",
    "        self.num_latents, self.latent_dim = latent_shape\n",
    "        self.attn_dropout = attn_dropout\n",
    "        self.num_heads = num_heads\n",
    "        self.num_freq_bands = num_freq_bands\n",
    "        \n",
    "        # randomly initialise latent\n",
    "        self.latent = nn.Parameter(torch.randn(self.num_latents, self.latent_dim))\n",
    "        \n",
    "        # encoder\n",
    "        self.encoder = AttentionEncoder(\n",
    "            input_channels=input_channels, \n",
    "            latent=self.latent, \n",
    "            input_axis=self.input_axis, \n",
    "            attn_dropout=attn_dropout, \n",
    "            num_heads=num_heads, \n",
    "            num_freq_bands=num_freq_bands\n",
    "        )\n",
    "        \n",
    "        # decoder\n",
    "        self.decoder = TabularDecoder(\n",
    "            latent_dim=self.latent_dim, \n",
    "            output_dim=input_channels\n",
    "        )\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # encode\n",
    "        x, self.latent = self.encoder(x, self.latent)\n",
    "        # decode, reconstructed x\n",
    "        rec_x = self.decoder(self.latent)\n",
    "        return rec_x\n",
    "    \n",
    "    def get_latent(self):\n",
    "        return self.latent"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T15:13:09.210809Z",
     "start_time": "2023-10-13T15:13:09.140155Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we need to think about tabular loss functions. Here, we can explore both reconstruction losses and contrastive losses. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "class TabularLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Reconstruction loss functions for tabular data. We use two types which are commonly used with continuous data: \n",
    "    - Mean squared error\n",
    "    - Constrastive loss, measured as cosine distance between the original and reconstructed data\n",
    "    We seek to minimise both objectives.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 method: str = \"mse\",\n",
    "                 reduction: str = \"mean\",\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        assert method in [\"mse\", \"contrastive\"], \"Loss type not recognised\"\n",
    "        self.loss_type = method\n",
    "        self.reduction = reduction\n",
    "        \n",
    "        if method == \"mse\":\n",
    "            self.loss = nn.MSELoss(reduction=reduction)\n",
    "        elif method == \"contrastive\":\n",
    "            self.loss = nn.CosineEmbeddingLoss(reduction=reduction)\n",
    "            \n",
    "    def __call__(self, x: torch.Tensor, rec_x: torch.Tensor):\n",
    "        return self.loss(x, rec_x)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T14:30:09.637479Z",
     "start_time": "2023-10-13T14:30:09.568919Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, we write a pre-training loop that we can use for pre-training across cancer sites. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2191 2922 1758\n"
     ]
    }
   ],
   "source": [
    "# get overlap between omic columns\n",
    "col1 = blca.omic_df.columns\n",
    "col2 = brca.omic_df.columns\n",
    "print(len(col1), len(col2), len(set(col1).intersection(col2)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T14:43:23.117615Z",
     "start_time": "2023-10-13T14:43:23.014444Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "(436, 2191)"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blca.omic_df.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T14:45:58.016135Z",
     "start_time": "2023-10-13T14:45:57.949089Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "2191"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blca.omic_df.shape[1]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T14:52:12.825500Z",
     "start_time": "2023-10-13T14:52:12.728257Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1] 4 0.1\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "empty(): argument 'size' must be tuple of SymInts, but found element of type list at pos 2",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[59], line 42\u001B[0m\n\u001B[1;32m     38\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m idx \u001B[38;5;241m%\u001B[39m \u001B[38;5;241m10\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m     39\u001B[0m                 \u001B[38;5;28mprint\u001B[39m(loss)\n\u001B[0;32m---> 42\u001B[0m \u001B[43mpretrain_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     43\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mblca\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[1;32m     44\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[59], line 8\u001B[0m, in \u001B[0;36mpretrain_loop\u001B[0;34m(data, batch_size)\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpretrain_loop\u001B[39m(\n\u001B[1;32m      4\u001B[0m         data: TCGADataset,\n\u001B[1;32m      5\u001B[0m         batch_size: \u001B[38;5;28mint\u001B[39m, \n\u001B[1;32m      6\u001B[0m     ):\n\u001B[0;32m----> 8\u001B[0m     model \u001B[38;5;241m=\u001B[39m \u001B[43mTabPretrainer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      9\u001B[0m \u001B[43m        \u001B[49m\u001B[43minput_channels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[1;32m     10\u001B[0m \u001B[43m        \u001B[49m\u001B[43minput_axis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[1;32m     11\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlatent_shape\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m256\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m32\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[1;32m     12\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattn_dropout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[1;32m     13\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnum_heads\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m4\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m     14\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnum_freq_bands\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m8\u001B[39;49m\n\u001B[1;32m     15\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     17\u001B[0m     loader \u001B[38;5;241m=\u001B[39m DataLoader(\n\u001B[1;32m     18\u001B[0m         data, \n\u001B[1;32m     19\u001B[0m         batch_size\u001B[38;5;241m=\u001B[39mbatch_size, \n\u001B[1;32m     20\u001B[0m         shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, \n\u001B[1;32m     21\u001B[0m         num_workers\u001B[38;5;241m=\u001B[39mmultiprocessing\u001B[38;5;241m.\u001B[39mcpu_count()\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m\n\u001B[1;32m     22\u001B[0m     )\n\u001B[1;32m     23\u001B[0m     optimizer \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39moptim\u001B[38;5;241m.\u001B[39mAdam(model\u001B[38;5;241m.\u001B[39mparameters(), lr\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1e-3\u001B[39m)\n",
      "Cell \u001B[0;32mIn[54], line 28\u001B[0m, in \u001B[0;36mTabPretrainer.__init__\u001B[0;34m(self, input_channels, latent_shape, input_axis, attn_dropout, num_heads, num_freq_bands)\u001B[0m\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlatent \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mParameter(torch\u001B[38;5;241m.\u001B[39mrandn(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_latents, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlatent_dim))\n\u001B[1;32m     27\u001B[0m \u001B[38;5;66;03m# encoder\u001B[39;00m\n\u001B[0;32m---> 28\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencoder \u001B[38;5;241m=\u001B[39m \u001B[43mAttentionEncoder\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     29\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_channels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_channels\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[1;32m     30\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlatent\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlatent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[1;32m     31\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_axis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minput_axis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[1;32m     32\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattn_dropout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattn_dropout\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[1;32m     33\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnum_heads\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_heads\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[1;32m     34\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnum_freq_bands\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_freq_bands\u001B[49m\n\u001B[1;32m     35\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     37\u001B[0m \u001B[38;5;66;03m# decoder\u001B[39;00m\n\u001B[1;32m     38\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdecoder \u001B[38;5;241m=\u001B[39m TabularDecoder(\n\u001B[1;32m     39\u001B[0m     latent_dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlatent_dim, \n\u001B[1;32m     40\u001B[0m     output_dim\u001B[38;5;241m=\u001B[39minput_channels\n\u001B[1;32m     41\u001B[0m )\n",
      "Cell \u001B[0;32mIn[57], line 34\u001B[0m, in \u001B[0;36mAttentionEncoder.__init__\u001B[0;34m(self, input_channels, latent, input_axis, attn_dropout, num_heads, num_freq_bands)\u001B[0m\n\u001B[1;32m     32\u001B[0m \u001B[38;5;66;03m# simple single attention unit\u001B[39;00m\n\u001B[1;32m     33\u001B[0m \u001B[38;5;28mprint\u001B[39m(latent_dim, input_dim, num_heads, attn_dropout)\n\u001B[0;32m---> 34\u001B[0m enc \u001B[38;5;241m=\u001B[39m PreNorm(latent_dim, \u001B[43mAttention\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlatent_dim\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minput_dim\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mheads\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_heads\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdim_head\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_heads\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdropout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattn_dropout\u001B[49m\u001B[43m)\u001B[49m, context_dim\u001B[38;5;241m=\u001B[39minput_dim)\n\u001B[1;32m     36\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayers \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mModuleList([enc])\n",
      "File \u001B[0;32m~/pycharm/healnet/healnet/models/healnet.py:125\u001B[0m, in \u001B[0;36mAttention.__init__\u001B[0;34m(self, query_dim, context_dim, heads, dim_head, dropout)\u001B[0m\n\u001B[1;32m    122\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mheads \u001B[38;5;241m=\u001B[39m heads\n\u001B[1;32m    124\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mto_q \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mLinear(query_dim, inner_dim, bias \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m--> 125\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mto_kv \u001B[38;5;241m=\u001B[39m \u001B[43mnn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mLinear\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcontext_dim\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minner_dim\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m    127\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mDropout(dropout)\n\u001B[1;32m    128\u001B[0m \u001B[38;5;66;03m# add leaky relu\u001B[39;00m\n",
      "File \u001B[0;32m~/.conda/envs/cognition/lib/python3.9/site-packages/torch/nn/modules/linear.py:96\u001B[0m, in \u001B[0;36mLinear.__init__\u001B[0;34m(self, in_features, out_features, bias, device, dtype)\u001B[0m\n\u001B[1;32m     94\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39min_features \u001B[38;5;241m=\u001B[39m in_features\n\u001B[1;32m     95\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mout_features \u001B[38;5;241m=\u001B[39m out_features\n\u001B[0;32m---> 96\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweight \u001B[38;5;241m=\u001B[39m Parameter(\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mempty\u001B[49m\u001B[43m(\u001B[49m\u001B[43m(\u001B[49m\u001B[43mout_features\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43min_features\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mfactory_kwargs\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m     97\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m bias:\n\u001B[1;32m     98\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbias \u001B[38;5;241m=\u001B[39m Parameter(torch\u001B[38;5;241m.\u001B[39mempty(out_features, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfactory_kwargs))\n",
      "\u001B[0;31mTypeError\u001B[0m: empty(): argument 'size' must be tuple of SymInts, but found element of type list at pos 2"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def pretrain_loop(\n",
    "        data: TCGADataset,\n",
    "        batch_size: int, \n",
    "    ):\n",
    "    \n",
    "    model = TabPretrainer(\n",
    "        input_channels=[1], \n",
    "        input_axis=[2], \n",
    "        latent_shape=[256, 32], \n",
    "        attn_dropout=0.1, \n",
    "        num_heads=4,\n",
    "        num_freq_bands=8\n",
    "    )\n",
    "    \n",
    "    loader = DataLoader(\n",
    "        data, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True, \n",
    "        num_workers=multiprocessing.cpu_count()-1\n",
    "    )\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    \n",
    "    loss_fn = TabularLoss(method=\"mse\")\n",
    "    \n",
    "    for epoch in tqdm(range(10)):\n",
    "        for idx, batch in enumerate(loader):\n",
    "            [omic], censorship, event_time, y_disc = batch\n",
    "            # return reconstructed omic data\n",
    "            rec_omic = model(omic)\n",
    "            loss = loss_fn(omic, rec_omic)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # print every 10th batch\n",
    "            if idx % 10 == 0:\n",
    "                print(loss)\n",
    "            \n",
    "    \n",
    "pretrain_loop(\n",
    "    data=blca, \n",
    "    batch_size=1)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T16:37:39.416953Z",
     "start_time": "2023-10-13T16:37:39.300162Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cognition",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
