data_path: /net/archive/export/tcga
tcga_path: /net/archive/export/tcga/tcga
gdc_client: /home/kh701/pycharm/x-perceiver/gdc-client
log_path: /home/kh701/pycharm/x-perceiver/logs
seed: 1

dataset: ucec
model: healnet # [fcnn, healnet, healnet_early]

n_folds: 1

wandb: True

data:
  # image preprocessing
  resize: True # ONLY use for debugging - manually resize images
  resize_height: 1024 # if resize is True
  resize_width: 1024 # if resize is True
  patch_size: 256 # don't changed if used for pre-processing
  wsi_level: 2 # WSI resolution level (if available)
  prefetch: False


task: survival # [survival, classification]
sources:
  - omic
  - slides


survival: # all parameters related to survival task
  loss: nll # valid: nll, ce_survival, cox
  subset: uncensored # subset used to calculate survival bin cutoffs

train_loop:
  checkpoint_interval: 5
  eval_interval: 5
  batch_size: 8
  epochs: 50

optimizer:
  max_lr: 0.005
  lr: 0.002
  momentum: 0.92
  weight_decay: 0.00026 # or None


model_params:
  # OMIC
#  output_dims: 4 # refers to n_classes for classification, n_bins for survival
#  class_weights: inverse # one of inverse, inverse_root, None; only relevant for classification, not survival
#  num_freq_bands: 2
#  depth: 1
#  max_freq: 2.
#  num_latents: 4
#  latent_dim: 4
#  cross_dim_head: 16
#  latent_dim_head: 16
#  cross_heads: 1
#  latent_heads: 8
#  attn_dropout: 0
#  ff_dropout: 0
#  fourier_encode_data: True
#  self_per_cross_attn: 1  # if 0, no self attention at all
#  weight_tie_layers: False # share weights between layers if False | KEEP THIS, otherwise model size is quite large

  # SLIDES
  output_dims: 4 # refers to n_classes for classification, n_bins for survival
  class_weights: inverse # one of inverse, inverse_root, None; only relevant for classification, not survival
  num_freq_bands: 5
  depth: 6
  max_freq: 2.
  num_latents: 120
  latent_dim: 100
  cross_dim_head: 80
  latent_dim_head: 128
  cross_heads: 1
  latent_heads: 8
  attn_dropout: 0.3
  ff_dropout: 0.3
  fourier_encode_data: True
  self_per_cross_attn: 1  # if 0, no self attention at all
  weight_tie_layers: False # share weights between layers if False | KEEP THIS, otherwise model size is quite large
